\documentclass[format=sigplan, review=false, screen=true, 10pt]{acmart}
\usepackage[T1]{fontenc}
\usepackage[utf8x]{inputenc}
%\usepackage{fontspec}
%\setmonofont[
%  Contextuals={Alternate},
%  Scale=0.9
%]{Fira Code}
\makeatletter
\def\verbatim@nolig@list{}
\makeatother
\usepackage{microtype}
\linespread{0.97}
\usepackage{balance}

\usepackage[icon=Note, color={0.6 0.8 0.9}]{pdfcomment}

\makeatletter
\let\origsection\section
\let\origsubsection\subsection
\renewcommand\section{\@ifstar{\starsection}{\nostarsection}}
\renewcommand\subsection{\@ifstar{\starsubsection}{\nostarsubsection}}

\newcommand\nostarsection[1]
{\sectionprelude\origsection{#1}\sectionpostlude}

\newcommand\starsection[1]
{\sectionprelude\origsection*{#1}\sectionpostlude}

\newcommand\nostarsubsection[1]
{\subsectionprelude\origsubsection{#1}\subsectionpostlude}

\newcommand\starsubsection[1]
{\subsectionprelude\origsubsection*{#1}\subsectionpostlude}

\newcommand\sectionprelude{%
  \vspace{-0.5em}
}

\newcommand\sectionpostlude{%
  \vspace{0em}
}

\newcommand\subsectionprelude{%
  \vspace{-0.25em}
}

\newcommand\subsectionpostlude{%
  \vspace{-0.05em}
}
\makeatother

\usepackage{booktabs} % For formal tables
\usepackage{bussproofs}
\usepackage{graphics}
\usepackage{xcolor}

\usepackage{minted}
\usemintedstyle{friendly}
\setminted{
    frame=none, %lines,
    framesep=2mm,
    baselinestretch=1.2,
    %bgcolor=lightgray,
    fontsize=\footnotesize,
    escapeinside=\#\#
    }

\usepackage{filecontents}

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\usepackage{listings}
\lstdefinestyle{mystyle}{
    %backgroundcolor=\color{backcolour},   
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    numberstyle=\tiny,
    stringstyle=\color{purple},
    basicstyle=\small\tt,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,
    frame=single,
    xleftmargin=1em,
    xrightmargin=1em,
    frame=shadowbox,
    rulesepcolor=\color{gray},
    tabsize=2
}
 
\lstset{style=mystyle,
  literate=
  {->} {$\to$} 2
  {<-} {$\leftarrow$} 2
  {=>} {$\Rightarrow$} 2
  {forall} {$\forall$} 1
  {exists} {$\exists$} 1
  {phi} {$\varphi$} 1
  {rho} {$\rho$} 1
  {kappa} {$\kappa$} 1
  {$nu$} {$\nu$} 1
  {$mu$} {$\mu$} 1
  {gamma} {$\gamma$} 1
  {subsetX} {$\subset$} 1
  {~>} {$\rightsquigarrow$} 2
  {<~>} {$\leftrightsquigarrow$} 3
  {elem} {$\in$} 1
}

\usepackage{cleveref}


%%% The following is specific to Haskell '18 and the paper
%%% 'Ghosts of Departed Proofs (Functional Pearl)'
%%% by Matt Noonan.
%%%
\setcopyright{acmlicensed}
\acmPrice{15.00}
\acmDOI{10.1145/3242744.3242755}
\acmYear{2018}
\copyrightyear{2018}
\acmISBN{978-1-4503-5835-4/18/09}
\acmConference[Haskell '18]{Proceedings of the 11th ACM SIGPLAN International Haskell Symposium}{September 27--28, 2018}{St. Louis, MO, USA}
\acmBooktitle{Proceedings of the 11th ACM SIGPLAN International Haskell Symposium (Haskell '18), September 27--28, 2018, St. Louis, MO, USA}

% Document starts
\begin{document}
% Title portion. Note the short title for running heads
\title[Ghosts of Departed Proofs]{Ghosts of Departed Proofs (Functional Pearl)}

\author{Matt Noonan}
\orcid{1234-5678-9012-3456}
\affiliation{%
  \institution{Kataskeue LLC, Input Output HK}
  \city{Ithaca}
  \state{NY}
  \postcode{14850}
  \country{USA}}
\email{mnoonan@kataskeue.com}


\begin{abstract}

  Library authors often are faced with a design choice: should a function with
  preconditions be implemented as a partial function, or by returning a failure
  condition on incorrect use? Neither option is ideal. Partial functions lead
  to frustrating run-time errors. Failure conditions must be checked
  at the use-site,
  placing an unfair tax on the users who have ensured that the function's
  preconditions were correctly met.
  
  In this paper, we introduce an API design concept called ``ghosts of departed
  proofs'' based on the following observation: sophisticated preconditions can be
  encoded in Haskell's type system with no run-time overhead, by using proofs
  that inhabit phantom type parameters attached to \texttt{newtype} wrappers.
  The user expresses correctness arguments by constructing proofs to inhabit
  these phantom types.
  Critically, this technique allows the
  library \emph{user} to decide when and how to validate that the API's preconditions
  are met.

  The ``ghosts of departed proofs'' approach to API design can achieve many of the benefits
  of dependent types and refinement types, yet only requires some minor and well-understood
  extensions to Haskell 2010. We demonstrate the utility of this approach
  through a series of case studies, showing how to enforce novel invariants for lists,
  maps, graphs, shared memory regions, and more.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011099.10011692</concept_id>
<concept_desc>Software and its engineering~Formal software verification</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011024.10011025</concept_id>
<concept_desc>Software and its engineering~Polymorphism</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011075.10011078</concept_id>
<concept_desc>Software and its engineering~Software design tradeoffs</concept_desc>
<concept_significance>100</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003790.10002990</concept_id>
<concept_desc>Theory of computation~Logic and verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Formal software verification}
\ccsdesc[300]{Software and its engineering~Polymorphism}
\ccsdesc[100]{Software and its engineering~Software design tradeoffs}
\ccsdesc[300]{Theory of computation~Logic and verification}
%
% End generated code
%


\keywords{API design, software engineering, formal methods, higher-rank types}

\maketitle

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{M. Noonan}

%\input{samplebody-journals}

\section{Introduction}
\begin{quote}
  [Rico Mariani] admonished us to think about how we can build platforms that lead developers to write great, high performance code such that developers just fall into doing the ``right thing''. That concept really resonated with me. It is the key point of good API design. We should build APIs that steer and point developers in the right direction.
  
  \hfill --- Brad Abrams \cite{pitofsuccess}
\end{quote}

What is the purpose of a powerful type system? One practical perspective is
that a type system provides a mechanism for enforcing program
invariants at compile time. The desire to encode increasingly
sophisticated program invariants has led to a vast expanse of research
on more complex type systems, including dependent types \cite{augustsson1998cayenne,bove2009dependent}, refinement types \cite{freeman1991refinement}, linear
types \cite{wadler1990linear}, and many more. But despite this menagerie of powerful
type systems, workaday Haskell programmers have already been able to encode
surprisingly sophisticated invariants using nothing more than a
few well-understood extensions to the Damas-Hindley-Milner type system.

An early success story is the \texttt{ST} monad, which allows pure
computations to make use of local, mutable state. A phantom type parameter
and a clever use of rank-2 types in the \texttt{ST} monad's API gives
a compile-time guarantee that the local mutable state is invisible from the outside,
and hence the resulting computation really \emph{is} pure. As we will see, this
trick is just the tip of a rather large iceberg.

In this paper, we will take the perspective of a library author, writing in
Haskell 2010 (plus a few battle-tested language extensions). As a library
author, our goal will be to design \emph{safe} APIs that are also \emph{ergonomic}
for the end user. ``Safe'' means that we want to prevent the user from causing a
run-time error. ``Ergonomic'' means that the correct use of our
API must not place an undue burden on the user.

\begin{filecontents*}{idioms.hs}
-- Unsafe API using non-total functions.
head :: [a] -> a
head xs = case xs of
  (x:_) -> x
  []    -> error "empty list!"

endpts = do
  putStrLn "Enter a non-empty list of integers:"  
  xs <- readLn
  if xs /= [] then return (head xs, head (reverse xs))
              else endpts
----------------------------------------------------------
-- Returning Maybe / Optional values. Safe, but requires
-- the caller to pattern-match on the Maybe at every use,
-- even when the list is known to be non-empty. Frustrated
-- users cannot be blamed for using `fromJust`!
headMay :: [a] -> Maybe a
headMay xs = case xs of
  (x:_) -> Just x
  []    -> Nothing

safeEndpts = do
  putStrLn "Enter a non-empty list of integers:"  
  xs <- readLn
  case headMay xs of
    Just x -> return (x, fromJust (headMay (reverse xs))
    _      -> safeEndpts
----------------------------------------------------------
-- "Ghosts of Departed Proofs". Safe. Does not return
-- an optional value; preconditions are checked early
-- and carried by "ghosts" (specialized phantom types).
rev_cons :: Proof (IsCons xs) -> Proof (IsCons (Rev xs))
gdpReverse :: ([a] ~~ xs) -> ([a] ~~ Rev xs)

gdpHead :: ([a] ~~ xs ::: IsCons xs) -> a
gdpHead xs = head (the xs) -- safe!

gdpEndpts = do
  putStrLn "Enter a non-empty list of integers:"  
  xs <- readLn
  name xs $ \xs -> case classify xs of
    IsCons proof ->
      return (gdpHead (xs            ...proof),
              gdpHead (gdpReverse xs ...rev_cons proof))
    IsNil proof  -> gdpEndpts
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{idioms.hs}
  \caption{Idioms for implementing the
    \texttt{head} function, along with usage examples.
    The \texttt{gdpHead} function can only be invoked by
    presenting a proof that the list is non-empty, combining the simplicity of the
    first example with the safety of the second. \texttt{rev\_cons}
    is a proof combinator exported by the library to help the
    user prove that the reverse of a non-empty list is also non-empty. See
    \cref{full-gdp} for details.\label{idioms-fig}}
\end{figure}

\subsection{Common idioms for handling pre-conditions}

No matter the language, a programmer often has to write functions
that place constraints on their input. For example, the venerable
\texttt{head} function will extract the first element of a list,
but asks its users to only give it a non-empty list to operate on.
Now put yourself in the shoes of \texttt{head}'s author: how can
you ensure that \texttt{head} will be used properly? Let us recount
a variety of strategies used in the wild.

\paragraph{Run-time failure on bad inputs.}
The simplest approach is
to have a function just fail on malformed inputs. The failure mode can
be an immediate run-time error (as in \texttt{head} from \Cref{idioms-fig}),
an exception, or undefined behavior (as in \texttt{C++}'s \texttt{std::vector<T>::front()}).
  
\paragraph{Returning a dummy value.}
  To avoid run-time errors, some APIs may have a ``dummy value''
  for indicating the result of a failed operation. For example, Common Lisp's
  \texttt{car} and \texttt{golang}'s \texttt{Front()} both return \texttt{nil}
  when passed an empty list. The caller must explicitly check for this dummy
  value. Other contortions may be needed if the container is also allowed to
  hold \texttt{nil}, to disambiguate between ``the input list is empty'' and
  ``\texttt{nil} is the first element of this list''.

\paragraph{Returning a value with an option type.}
  A related strategy
  for languages with stricter typing discipline is to use an ``option type,'' such
  as Haskell's \texttt{Maybe} or Scala's \texttt{Option}. A value of type \texttt{Maybe T}
  cannot be used where a value of type \texttt{T} was expected, so the user must
  explicitly pattern match on the optional value to extract the result and handle the
  error case. This approach may lead to frustration when the user believes that the
  error case is not possible, as when \texttt{headMay} is applied to the reverse of
  a non-empty list in \Cref{idioms-fig}.
  
\paragraph{Modifying input types to exclude bad inputs.}
Finally, the API designer may select more restrictive types for the inputs in order
to make the function total. For example, some Haskell libraries make use of the
\texttt{NonEmpty} type for lists that contain at least one element. The \texttt{head}
function then becomes total. The user 
can prove that their list is non-empty by making use of the smart constructor
\texttt{nonEmpty :: [a] -> Maybe (NonEmpty a)}.
The drawbacks include duplication
(do we re-implement \texttt{length} for \texttt{NonEmpty}?)
and awkwardness when encoding preconditions that relate
several inputs (\textit{e.g.} requiring two lists to have the same length).

\subsection{Leading the user into temptation}
The ``return-an-optional-value'' idiom is well-known and popular in the functional
programming world. The author of a library function that returns \texttt{Maybe a}
can certainly sleep well at night, content in the knowledge that their function
will never cause a run-time error.

But what about the \emph{users} of that library? Has the library author helped the user stay
on a virtuous path, or have they led the user into temptation?

In fact, the author of the library has merely pushed extra responsibility onto the user.
Every time the user applies a function that uses the optional-return idiom, they are obliged
to test the return value and handle the error case. Even worse, the user is still asked
to handle the error case when they have \emph{correctly} ensured that the function's
preconditions have been met! The library author sleeps well, while even the most vigilant
users are forced to toil against those impossible error cases.

No wonder so many well-meaning users reach for unsafe functions like \texttt{fromJust}!
They have already proved (to their own satisfaction) that the function is being used properly, so they rightly
feel justified in ignoring the error case entirely. But now we see how the user has been led into
a pit of despair: they have ended up with a program that is exactly as fragile as one where the library
author had used the run-time-failure idiom!\footnote{In fact, things are slightly \emph{worse}: we have also introduced a little
bit of extra allocation and indirection for creating and unpacking the return value in the non-error case.}
Even if the user has mentally constructed a proof that this specific use of \texttt{fromJust} is safe \emph{now},
who can say what will
happen as the software changes over time? Without tooling to ensure that the user's proof \emph{remains} valid,
the software is left in a brittle state.


For example, a recent snapshot of \texttt{hackage}
reveals over 2000 instances where the partial function
\texttt{fromJust} is applied to the
result of \texttt{Data.Map}'s \texttt{lookup}. Any one
of these instances may be a vignette of a programmer
falling into a pit of despair:
they had a mental proof that a certain key must be
present in the map, but possessed no mechanism for
\emph{communicating} that proof to the
\texttt{lookup} function. In frustration, they made the
pragmatic---but unsafe---decision to introduce partiality.

\subsection{Who is to blame?}
It would be easy to lay the blame at the foot of the the user.
After all, they were the ones who brought in partial functions!
But this perspective misses the point: when we return a \texttt{Maybe},
even a perfect user who has done their due diligence will be forced to handle an error case---exactly the
error case that they were so careful to avoid! The real problem is
that the conversion from a partial function to a \texttt{Maybe}-returning function is
a bit of a cheat on the part of the library author. Instead of
adding \texttt{Nothing} to a function's codomain, why not simply
restrict the function's domain to the set of valid inputs?
The user would still be responsible for ensuring that the inputs are valid but,
having done so, they would not be asked to introduce a spurious error handler.


\subsection{An alternative: Ghosts of Departed Proofs}
In the following sections, we will elaborate a design concept for
creating libraries that supports a dialogue between
 library and  user: the library can require that certain conditions
are met, and the user can explain how they have met those obligations.
The key features of this approach are as follows:
\paragraph{Properties and proofs are represented in code.}
  Proofs are concrete entities within the host language, and can
  be analyzed or audited independently. In the tradition of
  the Curry-Howard correspondence, propositions are represented
  by types, and the proof of a proposition will be a value of that type.
\paragraph{Proofs carried by phantom type parameters.}
  To ensure that proof-carrying code does not have a run-time cost, proofs will only
  be used to inhabit types that appear as \emph{phantom type variables} attached to
  \texttt{newtype} wrappers.
  The \texttt{newtype} wrapper is erased during compilation, leaving no run-time cost and
  no evidence of these proofs in the final executable.
  The phantom type parameter is only used as a mechanism for transmitting the
  ``ghost of a departed proof'' to the library API.
  The name ``ghost proof'' is meant to suggest the related concept of \emph{ghost variables} in software
  verification \cite{leavens1999jml}, and to emphasize the idea that the proof is non-corporeal: no
  artifacts related to the proof should ever be discernible from the compiler's output.
\paragraph{Library-controlled APIs to create proofs.}
  Library authors should retain control over how domain-relevant proofs can be created.
  That is, the library author should be the only one able to introduce new axioms about
  the behavior of their API.
  This may mean exporting functions that create values with known properties, or that
  classify a value into mutually disjoint refinements,
or that introduce existentially-quantified properties (\texttt{name} in \Cref{name-module}, \texttt{runSt} in \Cref{st-api}, or \texttt{withMap}
in \Cref{justified-api}).
\paragraph{Combinators for manipulating ghost proofs.}
  Libraries may export a selection of combinators so that the user can
  mix and match the evidence at hand to produce a satisfactory proof of a
  safety property. The goal is to enrich the vocabulary of the user, so
  that they can productively communicate their proofs to the library.


\subsection{The structure of this paper}
In this paper, we will use a series of case studies to show how library authors can use
ghosts of departed proofs (GDP) to create
APIs that are both \emph{safe} and \emph{ergonomic}: the
user cannot cause a run-time error when using the API, and incorrect uses
of the API will become compile-time errors. But the APIs must be straightforward
enough that the user is not tempted to subvert the library's safety guarantees by using
unsafe functions. Crucially, we want the user to be able to \emph{communicate} their
informal proofs to the library. If the user believes that a precondition has been met,
they should be able to explain \emph{why} to the library!

The GDP design concept is relatively simple to implement. Each case study includes example library code,
along with usage demonstrations. The examples in this paper are self-contained, and are bundled together in
a project suitable for further experimentation \cite{this}. The proof combinators and other machinery
from \Cref{full-gdp} are available as the \texttt{gdp} library on Hackage \cite{this-hackage}.

\subsection{A very short tutorial on safe coercions}
Several of the examples in this paper rely on a basic understanding of \emph{safe coercions}, a relatively recent addition to GHC Haskell \cite{Breitner:2014:SZC:2692915.2628141}. The details of safe coercions are a bit technical, but for the purposes of this paper it suffices to
know the following operational facts:
\begin{itemize}
\item The types \texttt{T} and \texttt{newtype N = N T} have the same run-time representation.
\item \texttt{coerce :: Coercible a b => a -> b} can be used as a zero-cost safe cast from \texttt{a}
  to \texttt{b}, whenever the \texttt{Coercible a b} constraint is satisfied.
\item If \texttt{N} is a \texttt{newtype} of \texttt{T}, then the constraints \texttt{Coercible N T}
  and \texttt{Coercible T N} hold in any module where the constructor of \texttt{N} is visible.
\end{itemize}
We will make repeated use of this last property to help enforce encapsulation. Suppose a library author creates a module
that defines \texttt{N} as a newtype of \texttt{T}, but \emph{does not} export the constructor. Then the
library author can use \texttt{coerce} to freely cast between \texttt{T} and \texttt{N}, but \emph{users}
of that library only see \texttt{N} as an opaque type, and are not able to coerce it to \texttt{T}.
\section{Case Study \#1: Sorted lists}

It is almost inevitable that a programmer will, at some point, be asked to work
with lists that have been sorted in one way or another. To ensure correctness,
the programmer may need to carefully manage various invariants, such
as ``all of these lists must have been sorted by the same comparator''. For a concrete
example, consider these \texttt{sortBy} and \texttt{mergeBy} functions:
\begin{minted}[frame=none]{haskell}
sortBy  :: (a -> a -> Ordering) -> [a] -> [a]

-- Usage constraint: in `mergeBy comp xs ys`, the
-- input lists `xs` and `ys` should also be sorted
-- by the same comparator `comp`.
mergeBy :: (a -> a -> Ordering) -> [a] -> [a] -> [a]
mergeBy comp xs ys = go xs ys
  where
    go []  ys' = ys'
    go xs' []  = xs'
    go (x:xs') (y:ys') = case comp x y of
      GT -> y : go (x:xs') ys'
      _  -> x : go xs' (y:ys')
\end{minted}
This efficient $O(n+m)$ implementation of \texttt{mergeBy} is easy to write,
but it comes with a hidden cost to the end user. Anybody who uses \texttt{mergeBy}
must ensure that the two input lists have been sorted by the same comparator.
If the user accidentally fails to sort the two inputs, or does not sort them in the same way,
\texttt{mergeBy} will quietly produce nonsense and introduce a subtle bug.

It would be possible to implement a version of \texttt{mergeBy} that
carefully inspected the inputs \texttt{xs} and \texttt{ys} as it
proceeded, and only produced a result if the inputs met the sorting
requirement. But this would impose a runtime cost on every use of
\texttt{mergeBy}, increase the complexity of its implementation,
and change the result type to \texttt{Maybe [a]}. And then what?
Most users of \texttt{mergeBy} would argue to themselves ``This is
absurd! I already know that I sorted the input lists properly. This
function will never result in \texttt{Nothing}.'' It would be hard
to blame the user when they reach for an  unsafe function like
\texttt{fromJust}.

Clearly, everybody loses out in the above scenario. The
library author is inconvenienced by the increased implementation complexity.
The user is inconvenienced by the decreased performance and the need to
pattern match on the result, even when they  already know the
outcome of that match. No wonder that the status quo is to prominently display
a stern warning in the documentation, admonishing
any user who tries to \texttt{mergeBy} what they didn't \texttt{sortBy}.

But what if the user really \emph{does} have proof that the input lists have
been sorted properly? Can we devise a mechanism that allows the user to communicate
this proof to \texttt{mergeBy}?


\begin{filecontents*}{named.hs}
module Named (Named, type (~~), name) where

import Data.Coerce

newtype Named name a = Named a
type a ~~ name = Named name a

-- Morally, the type of `name` is
--      a -> (exists name. (a ~~ name))
name :: a -> (forall name. (a ~~ name) -> t) -> t
name x k = k (coerce x)
\end{filecontents*}
\begin{figure}[b]
  \inputminted{haskell}{named.hs}
  \caption{A module for attaching ghostly names to values. The rank-2 type of \texttt{name},
    making use of a polymorphic continuation, is one way to emulate an existential type in
    Haskell. By hiding the constructor of \texttt{Named}, this module ensures that \texttt{name}
    is the only way to introduce a name for a value.
    %The \texttt{Defining} typeclass
    %leverages \texttt{GHC}'s magic \texttt{Coercible} class \cite{Breitner:2014:SZC:2692915.2628141},
    %ensuring that \texttt{defn} is available to a library's author but not
    %to the library's users. \Cref{lemma-demo} provides a usage example for \texttt{defn}.
    \label{name-module}}
\end{figure}
\subsection{Conjuring a name}

The first challenge is how to express the idea of two  comparators
being ``the same''. In a language that supports equality tests on functions,
you could imagine a solution where the \texttt{sortBy} function returns both the sorted
list and a reference to the comparator that was used; \texttt{mergeBy} could
then check that the comparators matched. But this has a run-time cost for carrying
around the comparator references, and it still would require \texttt{mergeBy} to
return \texttt{Nothing} if it was given bogus arguments.

A different solution, in line with the GDP concept, is to introduce a \texttt{newtype} wrapper equipped with
a phantom type parameter \texttt{name}.
In code, we will write this wrapper as \verb|a ~~ n|, to be read as
``values of type \texttt{a} with name \texttt{n}''. To ensure that there is no
run-time penalty for using names, \verb|a ~~ n| is implemented as a \texttt{newtype}
around \texttt{a}, with a phantom type parameter \texttt{n}. A simple module for named values
can be found in \Cref{name-module}; the key feature is the exported \texttt{name}
function that expresses the concept ``any value can be given a name''.

To emulate an existentially-quantified type in Haskell, we will have to jump through
a small hoop with \texttt{name}. Instead of directly returning a value with a name attached,
\texttt{name} says to the user ``tell me what you wanted to do with that named value,
and I'll do it for you''. This slight-of-hand is responsible for the rank-2
signature of \texttt{name}. The user must hand \texttt{name} a computation that is
entirely agnostic about the name that will be chosen. More on this point in \cref{ghost-danger}.
%In \cref{case4}, we will extend this module with extra functionality in support of
%custom, library-defined names.

Once we have introduced names, it becomes handy to have a uniform way of stripping names
and other phantom data from a value. We do this with a simple two-parameter typeclass,
like so:
\begin{minted}[frame=none]{haskell}
class The d a | d -> a where
    the :: d -> a
    default the :: Coercible d a => d -> a
    the = coerce
\end{minted}
By using this default signature for \texttt{the}, most instances of \texttt{The}
can be declared with an empty body:
\begin{minted}[frame=none]{haskell}
instance The (a ~~ name) a
\end{minted}
The default method's use of a safe coercion helps ensure that forgetting a value's name 
incurs no run-time cost.

\begin{filecontents*}{ordered.hs}
module Sorted (Named, SortedBy, sortBy, mergeBy) where   

import The
import Named

import           Data.Coerce
import qualified Data.List       as L
import qualified Data.List.Utils as U

newtype SortedBy comp a = SortedBy a
instance The (SortedBy comp a) a
  
sortBy :: ((a -> a -> Ordering) ~~ comp)
       -> [a]
       -> SortedBy comp [a]
sortBy comp xs = coerce (L.sortBy (the comp) xs)

mergeBy :: ((a -> a -> Ordering) ~~ comp)
        -> SortedBy comp [a]
        -> SortedBy comp [a]
        -> SortedBy comp [a]
mergeBy comp xs ys =
  coerce (U.mergeBy (the comp) (the xs) (the ys))        
\end{filecontents*}

\begin{filecontents*}{usageO.hs}
import Sorted
import Named
main = do
  xs <- readLn :: IO [Int]
  ys <- readLn
  name (comparing Down) $ \gt -> do
    let xs' = sortBy gt xs
        ys' = sortBy gt ys
    print (the (mergeBy gt xs' ys'))
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{ordered.hs}
  \caption{A module for working with lists that have been sorted by an arbitrary
    comparator. The refinement \texttt{SortedBy comp} is used to denote values that
    have been sorted by the comparator named \texttt{comp}.\label{sorted-module}}
\end{figure}


\begin{figure}
  \vspace{1em}
  \inputminted{haskell}{usageO.hs}
  \caption{Using the module developed in \Cref{sorted-module}. For types with an \texttt{Ord}
    instance, \texttt{comparing Down} produces a comparator for \texttt{(>)} that sorts in the
    opposite of the usual order.\label{sorted-module-demo}}
  \end{figure}

\subsection{Implementing a safe API for sorting and merging}
Now that we know how to attach ghostly names to values, we can tackle the design of a
safe and ergonomic interface to \texttt{mergeBy}. In \Cref{sorted-module}, we begin by defining
a \texttt{newtype} wrapper \texttt{SortedBy comp} that represents the predicate
``$x$ has been sorted by the comparator named \texttt{comp}''. The
wrapper's meaning is imbued by the type of \texttt{sortBy}, which takes a \emph{named}
comparator and a list, and produces a list that has been \texttt{SortedBy comp}.
Note that by \emph{not} exporting \texttt{SortedBy}'s constructor, we have ensured that
the \emph{only} way to obtain a value of type \texttt{SortedBy comp [a]} is through the
\texttt{sortBy} or \texttt{mergeBy} functions. The user is not allowed to assert that a
list is \texttt{SortedBy comp} by fiat.

The implementation is straightforward enough: we use \texttt{the} to coerce away the name of the comparator,
apply the simpler version of \texttt{sortBy} from \texttt{Data.List}, and then
introduce the \texttt{SortedBy comp} predicate by coercing the result. Since the coercions have
no run-time effect, the code generated by the compiler for our GDP-style \texttt{sortBy} is
simply a call to \texttt{Data.List}'s \texttt{sortBy}!

Similarly, the generated code for our \texttt{mergeBy} will just call the ``normal'' \texttt{mergeBy}.
But notice the argument types of the GDP-style \texttt{mergeBy} in \Cref{sorted-module}. The user
must hand \texttt{mergeBy} a named comparator, plus two lists that have been sorted by that very same
comparator. No stern warnings in the documentation are required: if the user tries to \texttt{mergeBy}
what they didn't \texttt{sortBy}, the program will simply fail to compile!

We have successfully developed a safe API for \texttt{sortBy} and \texttt{mergeBy}, but how ergonomic
is it? A usage example appears in \Cref{sorted-module-demo}. The program is almost identical to one
that uses the standard versions of \texttt{sortBy} and \texttt{mergeBy}, except for the line where
we attach a ghostly name to \texttt{comparing Down}. We are asking very little more from the user,
yet end up with an API that cannot be used incorrectly.

\subsection{Applications to user code}
Although the library author retains control over the \emph{introduction} of ghost proofs, the
user is still able to leverage these proofs for their own purposes, beyond the library author's original
design. For example, the user can write a simple function that extracts the minimal element of
a list with respect to a given comparator:
\begin{minted}[frame=none]{haskell}
minimum_O1 :: SortedBy comp [a] -> Maybe a
minimum_O1 xs = case the xs of
    []    -> Nothing
    (x:_) -> Just x
\end{minted}
Thanks to the meaning given to \texttt{SortedBy comp} by the \texttt{Sorted} API, this user-defined
function offers a strong guarantee that it can only be called on a sorted list. Despite being user-defined,
this function cannot be used incorrectly. Did you forget to sort the list before calling \texttt{minimum\_O1}?
Then your program will not compile.

\subsection{Aside: On the danger of naming a ghost}\label{ghost-danger}

Let us return for a moment to the somewhat unusual type of \texttt{name} in \Cref{name-module}.
Is all of this business about rank-2 types \emph{really} necessary, or is it merely ivory tower
bloviation?  You may well wonder, why not just have a function with a simple type like this:

\begin{minted}[frame=none]{haskell}
any_name :: a -> (a ~~ name)
any_name = coerce
\end{minted}

At its core, the question is really about \emph{who gets to choose} what \texttt{name} will be.
In the signature of \texttt{any\_name}, the \emph{caller} gets to select the types \texttt{a}
and \texttt{name}. In particular, they can attach any name they would like!
If that still does not sound so bad, consider this code:

\begin{minted}[frame=none]{haskell}
data Simon
  
up, down :: (Int -> Int -> Ordering) ~~ Simon
up   = any_name compare
down = any_name (comparing Down)

list1 = sortBy up   [1,2,3]
list2 = sortBy down [1,2,3]

merged = the (mergeBy up list1 list2) :: [Int]
-- [1,2,3,3,2,1]
\end{minted}
\noindent
The user has decided to name two different functions \texttt{Simon}, subverting the
guarantees offered by the API of the \texttt{Sorted} module. It is dangerous to
name a ghost!

Now compare this to the analogous program, using \texttt{name} instead of \texttt{any\_name}:
\begin{minted}[frame=none]{haskell}
name compare $ \up ->
  name (comparing Down) $ \down ->
    let list1 = sortBy up   [1,2,3 :: Int]
        list2 = sortBy down [1,2,3]
    in the (mergeBy up list1 list2)
\end{minted}
\noindent
Attempting to compile this program results in a type error:

\begin{lstlisting}
Couldn't match type "name1" with "name"
      ...
  Expected type: SortedBy name [Int]
    Actual type: SortedBy name1 [Int]
\end{lstlisting}
\noindent
What is the critical difference between these two examples? In the first, a user is
allowed to \emph{create} a named value by fiat. In the second, the user is only allowed to \emph{consume} a named value, by
providing a polymorphic function that can work with \emph{any} named value. The library's API provides
a helper function---in this case, \texttt{name}---for applying the consumer to a normal, unnamed value.
In practice, it is as if the
library has a secret supply of names, and selects one to use in a manner that is not
predictable (or even inspectable!) to the user.

%A general rule of thumb for library authors is:
%\emph{a ghost should not appear in the return type,  unless it also appears in an argument's type}. This simple rule ensures that
%the user of the library will not be allowed to materialize ghosts out of thin air.

\section{Case Study \#2: Sharing state threads}
The trick for using rank-2 types to conjure names outside of the user's control was
inspired by the \texttt{ST} monad and its rank-2 \texttt{runST :: (forall s. ST s a) -> a}
function \cite{launchbury1994lazy}. In this brief case-study, we elaborate the connection
between the \texttt{ST} monad and GDP-style names. The new perspective suggests novel
extensions to the \texttt{ST} API.
In \Cref{st-api} we recall the basic \texttt{ST} API \cite{launchbury1994lazy}, writing \texttt{St} to
disambiguate our version from the existing type in \texttt{Control.Monad.ST}.

In their safety analysis of the \texttt{ST} monad, Timany \textit{et al.} proposed to think of the \texttt{s} parameter as
representing a name attached to a region of the heap \cite{timany2017logical}.
We can think of \texttt{ST s} as acting like some kind of informal \texttt{State} monad over
\emph{named regions}, like in this Haskell-ish pseudocode:
\begin{minted}[frame=none]{haskell}
data Region = Region
type St s a = State (Region ~~ s) a

runSt :: (forall s. St s a) -> a
runSt action = name Region (evalState action)
\end{minted}

The notion of treating the \texttt{ST} monad's phantom type as a region name immediately leads to ideas for
other primitives. Once we can name regions, why not go on to invent more detailed names to describe
the minute contours of those regions? For example, let us see what happens if we add a binary type constructor $\cap$ so that \texttt{s $\cap$ s'}
names the region at the intersection of \texttt{s} and \texttt{s'}. We are quickly led to an API similar to \Cref{st-sharing-api} that
supports a new capability: individual sub-computations, at their discretion, may decide to share mutable reference cells with other sub-computations.



\begin{filecontents*}{st1.hs}
runSt    :: (forall s. St s a) -> a

newRef   :: a -> St s (a #$\in$# s)
readRef  :: (a #$\in$# s) -> St s a
writeRef :: (a #$\in$# s) -> a -> St s ()
\end{filecontents*}

\begin{filecontents*}{st2.hs}
runSt2 :: (forall s s'. St (s #$\cap$# s') a) -> a

liftL :: St s a -> St (s #$\cap$# s') a
liftR :: St s' a -> St (s #$\cap$# s') a

share :: (a #$\in$# s) -> St s (a #$\in$# (s #$\cap$# s'))

use  :: (a #$\in$# (s #$\cap$# s')) -> (a #$\in$# s)
symm :: (a #$\in$# (s #$\cap$# s')) -> (a #$\in$# (s' #$\cap$# s))
\end{filecontents*}

\begin{figure}[t]
  \inputminted{haskell}{st1.hs}
  \caption{The standard ``state thread'' API. We write \texttt{a $\in$ s} to
    denote a reference cell of type \texttt{a} in the memory region named \texttt{s}.
    In \texttt{Control.Monad.ST}, we would write \texttt{a $\in$ s} as
    \texttt{STRef s a}.\label{st-api}}
\end{figure}

\begin{figure}
  \inputminted{haskell}{st2.hs}
  \caption{Extending the state thread API with shared references.\label{st-sharing-api}}
\end{figure}

\begin{filecontents*}{st.hs}
stSharingDemo :: Bool
stSharingDemo = runSt2 $ do
  -- In the "left" memory region, create and return
  -- two references; one shared, and one not shared.
  (secret, ref) <- liftL $ do
      unshared <- newRef 42
      shared   <- share =<< newRef 17
      return (unshared, shared)
  -- In the "right" memory region, mutate the shared
  -- reference. If we attempt to access the non-shared
  -- reference here, the program will not compile.
  liftR $ do
      let mine = use (symm ref)
      x <- readRef mine
      writeRef mine (x + 1)
  -- Back in the "left" memory region, verify that the
  -- unshared reference still holds its original value.
  liftL $ do
      check <- readRef secret
      return (check == 42)
\end{filecontents*}

\begin{figure}
  %\vspace{1.5em}
  \inputminted{haskell}{st.hs}
  \caption{An \texttt{ST}-style pure computation using local mutable
    references. 
    Although the \texttt{secret} reference is in scope during the calculation
    in the ``right'' region, any attempted access will fail to compile.\label{st-example}}
\end{figure}

In effect, \texttt{runSt2} lets the user run a computation that makes use of
two partially-overlapping memory regions. Within that computation, the user
can run sub-computations bound to one or the other memory region. Furthermore,
a sub-computation can move any variable that it owns into the common overlap
via \texttt{share}. An example is shown in \Cref{st-example}, where one sub-computation
creates two cells: one private, and the other shared. A second sub-computation has unconstrained
access to the shared cell. Yet even though the private reference is also in scope during
the second sub-computation, any attempts to access it there will fail to compile.

\section{Case Study \#3: Key-value lookups}

It is not uncommon to find algorithms based around key-value maps that rely
on certain keys being present at critical moments. For example, an evaluator for
well-scoped expressions may maintain a symbol table, subject to the
invariant that any symbol found at an expression node should have a corresponding entry in the
symbol table. It may be awkward to write ``missing symbol'' handlers into the expression
evaluator---doubly so if the API was designed so that missing symbols are supposed to
be impossible.

In this case study, we will see how to use the GDP concept to build an API where the
user can express the thought ``\emph{this} key must be present in \emph{that} map.''
In the process, we will vindicate  some of those
2000 moments, forever enshrined on Hackage, where a programmer fell into
the pit of despair and followed a map lookup by \texttt{fromJust}.

\Cref{justified-api} gives a small, GDP-style API based on the author's
\texttt{justified-containers}
package\footnote{In fact, the GDP technique was developed in order to generalize the
  design of \texttt{justified-containers} to other domains, and to add flexibility
  for the end-user by providing a wider selection proof combinators.}. The key features are:
\begin{itemize}
\item A predicate \texttt{Key ks}, meaning ``belongs to the key set named \texttt{ks}.''
  A value of type \texttt{Key ks k} is a value of type \texttt{k}, with a ghost proof
  that it is present in the key set named \texttt{ks}.
\item A predicate \texttt{JMap ks}, meaning ``has a key set named \texttt{ks}.'' A value
  of type \texttt{JMap ks k v} is a \texttt{Map k v}, with a key set named \texttt{ks}.
\item The rank-2 \texttt{withMap} function, analogous to \texttt{name}, that attaches
  a ghostly key set to a map. This function encodes the notion that any map has
  \emph{some} set of keys, perhaps not known to us at compile time.
\item The \texttt{member} function. This function checks if a key is present in a map
  with key set \texttt{ks} and,
  if so, produces a ghost proof of that fact using \texttt{Key ks}.
\item Finally, the function \texttt{lookup}. This function is total because the
  key carries a ghost proof that it is present in the map. As a result, \texttt{lookup}
  can safely return
  a \texttt{v} instead of a \texttt{Maybe v}, with no fear of run-time failure.
\end{itemize}

Note that proving a key can be found in a certain map does not mean it can \emph{only}
be found in that map! In \Cref{justified-usage}, we see that some evidence can be re-used:
we can find the same key in a whole variety of maps.

\subsection{Designing for the user's state of knowledge}
It is instructive to compare the two \texttt{lookup} types \texttt{k -> Map k v -> Maybe v}
and \texttt{Key ks k -> JMap ks k v -> v}. We do not intend to claim that
one of these is better than the other. Instead, the claim is much simpler: these two functions
\emph{reflect different expectations about the user's knowledge}.

If the user legitimately does not know whether or not a key is present, then the
\texttt{Maybe}-returning \texttt{lookup} is entirely appropriate. The user's incomplete knowledge
about the result of the operation is exactly reflected in the return type, so they will
not feel inconvenienced by the need to handle both the \texttt{Just v} (key present)
and \texttt{Nothing} (key absent) cases.

On the other hand, if the user already believes the key should be present based on some
external evidence, then they will be happier writing a program that does not need to handle
the impossible missing-key state. But to ensure safety, they must communicate that evidence to the library
somehow; here, via the \texttt{Key ks} predicate.

\subsection{Application: well-formed adjacency lists}

The power of this method becomes more apparent when considering maps where
the values are expected to reference the keys in some way. Consider this
simple adjacency representation for directed graphs that maps each vertex to its list of immediate neighbors:
\begin{minted}[frame=none]{haskell}
type Digraph v = Map v [v]
\end{minted}
Well-formed \texttt{Digraph}s
should satisfy the property that every vertex referenced in any neighbor list is also
a valid key in the adjacency map.

Traditionally, graph APIs that use adjacency representations require well-formed
graphs, but make it the user's responsibility to ensure well-formedness. For example,
the \texttt{Data.Graph} API from \texttt{containers} has a constructor that
will silently discard edges whose targets do not appear in the node list.
%%In \texttt{C++}, \texttt{boost::adjacency\_list} has an \texttt{add\_edge} function
%%that returns a \texttt{bool}. This value must be

Our GDP-style API for maps gives us a vocabulary for
translating the notion ``a well-formed adjacency list'' into a program invariant that can
be checked by the compiler. We simply write what we mean: a well-formed adjacency
map should map each vertex to a list of vertices that are keys of that \emph{same} map.
In other words:
\begin{minted}[frame=none]{haskell}
type Digraph vs v = JMap vs v [Key vs v]
\end{minted}
With the help of this type, a user can now enforce the invariant ``this adjacency map
must be well-formed'' at compile time. A similar strategy can be used to eliminate a
whole class of bugs when using symbol tables, evaluation contexts,
database models, and or any other data structure based around a recursive key-value store.

\begin{filecontents*}{justified.hs}
newtype JMap ks k v = JMap (Map k v) deriving Functor
newtype Key  ks k   = Key k

instance The (JMap ks k v) (Map k v)
instance The (Key ks k) k

member   :: k -> JMap ks k v -> Maybe (Key ks k)
lookup   :: Key ks k -> JMap ks k v -> v
reinsert :: Key ks k -> v -> JMap ks k v -> JMap ks k v
withMap  :: Map k v -> (forall ks. JMap ks k v -> t) -> t
\end{filecontents*}

\begin{filecontents*}{justified-usage.hs}
test = Map.fromList [ (1, "Hello"), (2, "world!") ]

withMap test $ \table ->
  case member 1 table of
    Nothing  -> putStrLn "Missing key!"
    Just key -> do
      let table'  = reinsert key "Howdy" table
          table'' = fmap (map upper) table
      putStrLn ("Value in map 1: " ++ lookup key table)
      putStrLn ("Value in map 2: " ++ lookup key table')
      putStrLn ("Value in map 3: " ++ lookup key table'')
-- Output:
--   Value in map 1: Hello
--   Value in map 2: Howdy
--   Value in map 3: HELLO
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{justified.hs}
  \caption{A fragment of the API from \texttt{justified-containers}.
    The GDP-style predicates \texttt{Key ks k} and \texttt{JMap ks k v} are used to represent
    ``a value of type \texttt{k} belonging to the set \texttt{ks}'' and ``a map with key set \texttt{ks}'',
    respectively. \label{justified-api}}
\end{figure}
\begin{figure}
  \inputminted{haskell}{justified-usage.hs}
  \caption{A usage example for the API in \Cref{justified-api}. The \texttt{member} function is used
    to check if a key is present in \texttt{table}; within the scope of the \texttt{Just} case, \texttt{key}
    carries a phantom proof of its presence in \texttt{table}. The same phantom proof can also be used as evidence that
    \texttt{key} is present certain other maps as well, such as \texttt{table'} (\texttt{table} with a
    value changed) and \texttt{table''} (\texttt{table} modified by \texttt{fmap}).\label{justified-usage}} 
\end{figure}


\subsection{Changing the key set}\label{changing-keys}
But what about maps that are related, yet do not have exactly the same key sets?
As a concrete example, consider the \texttt{insert} function. Although \texttt{insert} will usually
modify the key set of a map, we still know quite a lot about the keys in the updated map.
 Imagine you were a user, in possession of a key and a proof
that it is present in the original map. It would be quite frustrating if  we were
unable to use that same key freely in the expanded map!
The library author, anticipating this need, should provide a proof combinator that
converts a proof of ``\texttt{k} is a valid key of \texttt{m}'' into a proof of
``\texttt{k} is a valid key of \texttt{insert k' v m}.''

To support this use-case, \texttt{justified-containers} provides the rank-2
function \texttt{inserting}:
\begin{minted}[frame=none]{haskell}
inserting :: Ord k => k -> v -> JMap ks k v
          -> (forall ks'. JMap ks' k v
                       -> (Key ks k -> Key ks' k)
                       -> Key ks' k
                       -> t)
          -> t
\end{minted}
Since insertion results in a map with a new key set, we must
introduce the ghost of this new key set inside another \texttt{forall}.
But what are the other parameters being passed to the continuation?
They form a collection of evidence and proof combinators that the user may
need to formulate a safety argument. Concretely,  the continuation has access to:
\begin{enumerate}
\item The updated map, of type \texttt{JMap ks' k v}. The phantom type \texttt{ks'}
  represents the key set \texttt{ks}, updated with the newly-inserted key.
\item A function that represents the inclusion of \texttt{ks} into \texttt{ks'}.
  The user can apply this function to convert a proof that a certain key belonged to the
  old map (a value of type \texttt{Key ks k}) into a proof that the key also belongs to the new map (a value of type \texttt{Key ks' k}).
\item Evidence that the inserted key is present in the new key set.
\end{enumerate}

The library author must perform a balancing act here. They should give 
the user an ample supply of evidence and proof combinators to support the user's
arguments, but just what and how much? For example, the user may well want to argue that a
every key \emph{other} than the new one is also present in the original map,
 but the API provides no straightforward way to do this.
It is also somewhat awkward to introduce yet \emph{another}
rank-2 function to the API.

In the final case study, we will investigate how
the library author can separate API functions from the lemmas \emph{about} those
functions, and in the process remove the need for some of these additional
rank-2 functions.

\section{Case Study \#4: Arbitrary invariants}\label{full-gdp}

\begin{filecontents*}{pred.hs}
-- Type exported, constructor hidden (but see `axiom`)
data Proof p = QED

-- Attaching proofs to values
newtype a ::: p = SuchThat a

(...) :: a -> Proof p -> (a ::: p)
x ...proof = coerce x

-- Logical constants. We can use empty data declarations,
-- because these types are only used as phantoms.
data TRUE
data FALSE
data p && q
data p || q
data p --> q
data Not p
data p == q

-- Natural deduction rules (implementations all
-- ignore parameters and return `QED`)
andIntro    :: Proof p -> Proof q   -> Proof (p && q)
andElimL    :: Proof (p && q)       -> Proof p
orIntroL    :: Proof p              -> Proof (p || q)
implIntro   :: (Proof p -> Proof q) -> Proof (p --> q)
implElim    :: Proof (p --> q) -> Proof p -> Proof q
notIntro    :: (Proof p -> Proof FALSE)   -> Proof (Not p)
contradicts :: Proof p -> Proof (Not p)  -> Proof FALSE
absurd      :: Proof FALSE               -> Proof p
refl        :: Proof (x == x)
         -- ... and many more
-- Exported function that allows library authors to
-- assert arbitrary axioms about their API.
axiom :: Proof p
axiom = QED
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{pred.hs}
  \caption{Basic constants and functions for building up the ``proofs''
    in ``ghosts of departed proofs''. 
    \label{predicate-logic}}
\end{figure}


In the previous case studies, we saw how introducing names and predicates can
help us develop safe APIs that allow the user to express correctness proofs.
However, there are a few aspects that remained awkward.

First, we have several
ways that a name-like entity could be introduced: either via the \texttt{name} operator
itself, or through other library-defined rank-2 functions like \texttt{runSt2}, \texttt{withMap}, or \texttt{inserting}.
It would be nice if the same mechanism could be used for all of these cases.

Second, we made extensive use of ghostly proofs carried by phantom type parameters. But these
phantom types needed something to attach to, so we introduced various domain-specific \texttt{newtype}
wrappers (\texttt{SortedBy}, \texttt{$\in$},  \texttt{JMap}). Each library exported its own
idiosyncratic proof combinators for working with its \texttt{newtype} wrappers. It would be better to have
a uniform mechanism for expressing, carrying, and manipulating these proofs.

In this case study, we will consider what kind of APIs we could write if we separated
type-level names from the constraints we want to place on the named values.
For example, let us return to the \texttt{head} function. We want to ensure that the
user only calls \texttt{head} on a list \texttt{xs} with outer constructor \texttt{(:)} (``cons'').
To express this condition, we introduce one more \texttt{newtype} wrapper,
written \texttt{:::} and pronounced ``such that''. Altogether, the phrase 
\verb|(a ~~ n ::: p)| should be read ``a value of type \texttt{a}, named \texttt{n}, such
that condition \texttt{p} holds.''


We can now write, very explicitly, the requirement that our library places on the user
of \texttt{head}: the parameter, called \texttt{xs}, must have outermost constructor
\texttt{(:)}. So we simply introduce a predicate \texttt{IsCons} using an empty datatype,
and write down the definition of a GDP-style \texttt{head}:
\begin{minted}[frame=none]{haskell}
data IsNil  xs
data IsCons xs

head :: ([a] ~~ xs ::: IsCons xs) -> a
head xs = Prelude.head (the xs)
\end{minted}
The \texttt{(:::)} type is similar to the \texttt{Refined} type from the \texttt{refinement} library \cite{refined},
but it gains extra power when used together with names:
names are the mechanism that allows us to take predicates about \emph{specific} values and
encode them at the type level.
The type \verb|([a] ~~ xs ::: IsCons xs)|
becomes a statement about the particular list being passed to \texttt{head}.
The library user is now free to come up with a proof of \texttt{IsCons xs} in whatever way they please.


\begin{filecontents*}{ex1.hs}
-- API functions
reverse :: ([a] ~~ xs) -> ([a] ~~ Rev xs)
reverse xs = defn (Prelude.reverse (the xs))

length :: ([a] ~~ xs) -> (Int ~~ Length xs)
length xs = defn (Prelude.length (the xs))

zipWith :: (a -> b -> c)
         -> ([a] ~~ xs ::: Length xs == n)
         -> ([b] ~~ ys ::: Length ys == n)
         -> [c]
zipWith f xs ys = Prelude.zipWith f (the xs) (the ys)

-- Names for API functions
newtype Length xs = Length  Defn
newtype Rev    xs = Rev     Defn

-- Lemmas (all bodies are `axiom`)
rev_length :: Proof (Length (Rev xs) == Length xs)
rev_rev    :: Proof (Rev (Rev xs) == xs)
rev_cons   :: Proof (IsCons xs) -> Proof (IsCons (Rev xs))

data ListCase a xs = IsCons (Proof (IsCons xs))
                   | IsNil  (Proof (IsNil  xs))

classify :: ([a] ~~ xs) -> ListCase a xs
classify xs = case the xs of
  (_:_) -> IsCons axiom
  []    -> IsNil  axiom
\end{filecontents*}

\begin{filecontents*}{ex2.hs}
dot :: ([Double] ~~ vec1 ::: Length vec1 == n)
    -> ([Double] ~~ vec2 ::: Length vec2 == n)
    -> Double
dot vec1 vec2 = sum (zipWith (*) vec1 vec2)

-- Compute the dot product of a list with its reverse.
dot_rev :: [Double] -> Double
dot_rev xs = name xs $ \vec ->
  dot (vec ...refl) (reverse vec ...rev_length)
\end{filecontents*}

\subsection{Logical combinators for ghostly proofs}

\begin{filecontents*}{tableaux.hs}
proof1, proof2 :: Proof ((p --> q) --> (Not q --> Not p))
  
proof1 =
  implIntro $ \p2q ->
    implIntro $ \notq ->
      notIntro $ \p ->
        (implElim p2q p) `contradicts` notq

proof2 = tableaux
\end{filecontents*}

We now have a mechanism for encoding arbitrary properties as phantom types. But how will the user
create ghostly proofs to inhabit those phantom types?
We can begin with a very simple \texttt{Proof} type,
sporting a single phantom type parameter and exactly one non-bottom value:
\begin{minted}[frame=none]{haskell}
data Proof p = QED
\end{minted} 
From this humble beginning, we can encode all of the rules of natural deduction as functions that
produce terms of type \texttt{Proof p}.
\Cref{predicate-logic} gives a small taste of the basic syntax and encoded deduction rules.

Once we have constructed a proof of type \texttt{Proof p}, we can use the \texttt{(...)} combinator to attach that proof to a value of type \texttt{a}, producing a value of type \verb|(a ::: p)|. Note that \texttt{p} will often be a proof \emph{about} the wrapped value,
but that is not required! Any value can carry any proof; the only thing that links value to proof is the use of a common name.

\subsection{Naming library functions}
To help the user create domain-relevant proofs, a library author may wish to export a lemma such
as ``reversing a list twice gives the original list''.
To express this idea, it is not sufficient to have a name for ``the original list''. We must also be able to
name some of the library's functions. This observation motivates an extension to the \texttt{Named} module (\Cref{name-module}), adding these three items:

\begin{minted}[frame=none]{haskell}
-- module Named, continued:
data Defn = Defn -- Type exported, constructor hidden.

-- A constraint synonym that is expected to only be
-- available in the module where `f` is defined.
type Defining f = (Coercible Defn f, Coercible f Defn)

-- Allow library authors to define introduction rules for
-- names that they have defined. The coercion is only
-- possible since this function is in the Named module.
defn :: Defining f => a -> (a ~~ f)
defn = coerce
\end{minted}

The idea is that a library author can introduce a new name \texttt{X} by
defining \texttt{X} as a \texttt{newtype} alias of \texttt{Defn}. If the library author does not
export the constructor of \texttt{X}, then the constraint \texttt{Defining X} \emph{only}
holds in the module where \texttt{X} was defined. It follows that the \texttt{defn} function
can be used to attach the name \texttt{X} to an arbitrary value, but \emph{only} in the
module where \texttt{X} was defined. By exporting \texttt{defn} with the \texttt{Defining f}
constraint, the \texttt{Named} module allows library authors to introduce new names and axioms,
while \emph{users} remain safely restrained.

How does the library author use this mechanism to introduce new names and axioms in practice?
In the case of the list-reversing lemma, the author can write:
\begin{minted}[frame=none]{haskell}
newtype Rev xs = Rev Defn

reverse :: ([a] ~~ xs) -> ([a] ~~ Rev xs)
reverse xs = defn (Prelude.reverse (the xs))

rev_rev :: Proof (Rev (Rev xs) == xs)
rev_rev = axiom
\end{minted}
Note that, in contrast with \texttt{inserting} from the previous case study,
the lemmas \emph{about} a function stand on their own.
The library author can add any number of lemmas about \texttt{reverse}
without modifying its signature. Furthermore, it also becomes easy to
create lemmas that relate multiple functions, such as \verb|rev_length|
and \verb|rev_cons| in \Cref{lemma-demo}. A sample of client code for this
library appears in \Cref{dot-product}, where the user defines a dot product function that only can be applied to same-sized lists.
The user then supplies evidence to convince the compiler that the dot product of a list with its reverse is legal.

\paragraph{On the safety of \texttt{defn}} It is instructive to momentarily return to the Simon example of \Cref{ghost-danger}. Isn't \texttt{defn}
as bad as \texttt{any\_name}? There is certainly a danger, but only for the library \emph{author}
who must be very careful indeed about how \texttt{Simon} is introduced. The library
\emph{users} are still unable to name arbitrary values ``\texttt{Simon}'' merely by using \texttt{defn}, because
they do not possess the necessary \texttt{Defining Simon} constraint.

\begin{figure}[t]
    \inputminted{haskell}{ex1.hs}
    \caption{A GDP-style module for manipulating and reasoning about lists.
      A variety of lemmas are exported by the module, to provide the
      user with a rich set of building blocks for constructing safety proofs.
       \label{lemma-demo}}
\end{figure}
\begin{figure}
    \inputminted{haskell}{ex2.hs}
    \caption{A user-defined dot product function that can only be used on same-sized lists,
      and a usage example. In the implementation of \texttt{dot\_rev}, the user makes the use
      of \texttt{dot} well-typed by expressing a proof that
      \texttt{vec} and \texttt{reverse vec} have the same length.
      Note that \texttt{refl} and \texttt{rev\_length} are effectively axiom schemas, and unification with
      the type of \texttt{dot} selects the correct instances of these schemas.\label{dot-product}}
\end{figure}

\subsection{Building theory libraries}
In both the \texttt{St} and the \texttt{justified-containers} case studies,
the library author exported proof combinators that encoded basic facts about
the algebra of sets. Such redundancy is undesirable for the library authors,
who have to spend more time writing and testing, but also for the end user who has to remember dozens of
variations on the same basic proof combinators.

Luckily, it is simple to factor out axioms and deduction rules for specific theories
from the libraries that make use of them. For example, we could publish a small
library containing basic predicates and deduction rules about sets, such as:
\begin{minted}[frame=none]{haskell}
data xs #$\subseteq$# ys
subset_tr :: Proof (a #$\subseteq$# b) -> Proof (b #$\subseteq$# c) -> Proof (a #$\subseteq$# c)
subset_tr _ _ = axiom
\end{minted}
This same theory library could be used to reason about shared \texttt{St}
regions and about key sets for maps, already achieving code reuse within
the narrow confines of this paper's examples.

An extra level of confidence
can be obtained by splitting the theory library
into \texttt{Assumed} and \texttt{Derived} submodules.\footnote{We only have ``confidence''---not ``surety''---because, after all, Haskell's type system \emph{is} inconsistent as a logic. That does not render it useless for
  the evidence-pushing we need for GDP, however!}
 A small, core set of
 axioms are placed in the \texttt{Assumed} module and carefully audited for correctness.
 A larger set of lemmas, derived from these axioms using GDP
proof combinators, reside in the \texttt{Derived} submodule. This separation
allows the library author to build up a large collection of lemmas from a
small---and hopefully easy-to-verify---set of basic axioms.

\subsection{Ghosts on the outside, proofs on the inside}
Factoring common lemmas into theory modules helps ensure that
the module exports give a sound model of the structure they are
describing. The author only needs to check the validity of their own
theory module; others can then re-use that validation for free.

But how can the author of a theory library be confident that they wrote down
the correct lemmas in the first place? To increase confidence, the author of the
theory library can apply formal verification tools like
Liquid Haskell \cite{vazou2016liquid} or \texttt{hs-to-coq} \cite{spector2018total},
attempting to prove that the library presents a sound API. Note that the tooling is only
required by the author of the library;
the end-users of the theory library still use plain Haskell. A simple
demonstration using Liquid Haskell can be found in this paper's repository \cite{this}.


\begin{figure}[h]
  \inputminted{haskell}{tableaux.hs}
  \caption{Proving the same theorem in two different ways. The first proof
    uses the proof combinators from \Cref{predicate-logic}. The second
    proof uses a typechecker plugin, exposed through the \texttt{tableaux}
    function (\Cref{tactics}).
    \label{tableaux-example}}
\end{figure}

\subsection{Building custom proof tactics}\label{tactics}
For simple properties, the task of writing a proof is not too difficult. But for
more sophisticated properties, the deployment of \emph{proof tactics} becomes
crucial. A proof tactic is a search strategy for proofs, usually targeted at
proving one particular class of theorems. For example, the \texttt{Coq} tactic
\texttt{omega} is useful for proving theorems about arithmetic, while
\texttt{simpl} is useful for simplifying a complex goal.

Tactics are often designed with a specific domain in mind; to be most useful,
theory creators (and library authors) should be able to create their own tactics
when needed. 

One approach to providing custom tactics is to leverage \texttt{GHC}'s support for
\emph{type-checker plugins}. These plugins hook into \texttt{GHC}'s $\textsf{OutsideIn}(X)$
inference algorithm \cite{vytiniotis2011outsidein}, teaching it to solve
new kinds of type constraints.

As a proof-of-concept, we developed a simple typechecker plugin that implements
proof by analytic tableaux \cite{smullyan1995first} for propositional logic. This tactic
can verify the satisfiability of any
valid formula of propositional logic; the na\"ive implementation takes about
60 lines of Haskell, plus 150 lines of glue code to mediate between
the tableaux solver and \texttt{GHC}.

To trigger the custom tactic, we introduce an empty injective type family \cite{stolarek2015injective}---hidden from the user---and a single exported function \texttt{tableaux}:
\begin{minted}[frame=none]{haskell}
type family ProofByTableaux p = p' | p' -> p
tableaux :: ProofByTableaux p
tableaux = error "proof by analytic tableaux."
\end{minted}
Morally, we want to think of \texttt{ProofByTableaux p} as an alias for \texttt{p}.
The trick is that our plugin will first get a chance to check that the proposition \texttt{p} is
a valid formula of propositional logic. Only then will the plugin allow \texttt{GHC}
to replace \texttt{ProofByTableaux p} with \texttt{p}.

%% Our type-checker plugin gets a chance to intervene whenever \texttt{GHC}
%% meets a type equality constraint of the form \texttt{ProofByTableaux p $\sim$ p'}.
%% When such a constraint is found, the plugin performs the following tasks:
%% \begin{enumerate}
%% \item Convert the \emph{type} \texttt{p'} to a \emph{formula} $\Phi$ in propositional
%%   logic, introducing free variables for any subterms that are not built from the
%%   propositional logic type constructors \verb|&&|, \verb#||#, \verb|-->|, and \verb|Not|.
%% \item Invoke the solver for analytic tableaux, attempting to find an assignment of truth
%%   values to variables such that $\neg \Phi$ is true.
%% \item If the solver finds such a truth assignment, report an error: the user asked us
%%   to apply the tactic to prove $\Phi$, but the variable assignment demonstrated that
%%   $\Phi$ is actually \emph{false}.
%% \item If the solver finds that no such truth assignment exists, then we have proven that
%%   $\Phi$ is valid. Tell \texttt{GHC} to discharge the constraint \texttt{ProofByTableaux p $\sim$ p'}, replacing it with \texttt{p $\sim$ p'}.
%% \end{enumerate}
For the user, the effect appears to be that \texttt{tableaux} can act as a value of
type \texttt{Proof p} whenever \texttt{p} is a valid formula in propositional logic.
A glance at \Cref{tableaux-example} demonstrates why proof tactics are so desirable:
the user can just wave their hands and say ``this is true by basic facts from propositional logic,''
instead of constructing a tedious proof by hand.

\begin{filecontents*}{implicit.hs}
head :: Fact (IsCons xs) => ([a] ~~ xs) -> a
head xs = Prelude.head (the xs)

gdpEndpts' = do
  putStrLn "Enter a non-empty list of integers:"  
  xs <- readLn
  name xs $ \xs -> case classify' xs of
    Cons -> using rev_cons xs $
      return (head xs, head (reverse xs))
    Nil  -> gdpEndpts'
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{implicit.hs}
  \caption{The original example from \Cref{idioms-fig}, using \texttt{reflection} to
    implicitly propagate \texttt{Proof}s to their use-sites.
    \label{implicit}}
\end{figure}

\subsection{Using reflection to pass implicit proofs}
The \texttt{reflection} library is an implementation of Kiselyov and Shan's functional pearl about implicit configurations \cite{kiselyov2004functional}, allowing the user to pass values implicitly using Haskell's typeclass machinery.
We can combine \texttt{reflection} with GDP in order to pass \emph{proofs} implicitly, so they seem to magically appear
right at the moment they are required.
 The relevant part of the \texttt{reflection}
API consists of two functions: \texttt{give} lets the user make a proof implicit, and \texttt{given}
recalls an implicit proof from the current context:
\begin{minted}[frame=none]{haskell}
type Fact p = Given (Proof p)  -- a useful constraint synonym
give  :: Proof a -> (Fact a => t) -> t
given :: Fact a => Proof a
\end{minted}

The \texttt{using} combinator is useful for applying an implication to the implicit context. The
named parameter \texttt{x} is used to help select the right proof from the context.
\begin{minted}[frame=none]{haskell}
using :: Fact (p n) =>
   (Proof (p n) -> Proof q) -> (a ~~ n) -> (Fact q => t) -> t
using impl x = give (impl given)
\end{minted}

Pattern-matching on a GADT constructor can bring new instances into scope;
this can be used as a mechanism for automatically introducing \texttt{Fact (IsCons xs)} inside the cons branch of
a pattern-match:

\begin{minted}[frame=none]{haskell}
data ListCase' a xs where
  Cons :: Fact (IsCons xs) => ListCase' a xs
  Nil  :: Fact (IsNil  xs) => ListCase' a xs

classify' :: forall a xs. ([a] ~~ xs) -> ListCase' a xs
classify' xs = case the xs of
  (_:_) -> give (axiom :: Proof (IsCons xs)) Cons
  []    -> give (axiom :: Proof (IsNil  xs)) Nil
\end{minted}

%This approach can save the end-user from the hassle of manually moving evidence from one ghost to another.
It is easy to see the benefit when comparing \Cref{implicit} with the original GDP example in \Cref{idioms-fig}.
 On the other hand, there is a small amount of overhead due to the passing of
typeclass dictionaries for \texttt{Fact}s, and it is not always easy to extract the right
proof from the implicit context without adding type annotations.

\section{Related work}
Phantom type parameters have several well-known applications in API design, supporting typed
embedded domain-specific languages (EDSLs) \cite{Leijen:1999:DSE:1267936.1267945}, pointer subtyping \cite{Leijen:2004:WPC:1017472.1017483}, and access control policies \cite{Fluet:2006:PTS:1180085.1180088}. Most of these applications
rely on monomorphic or universally-quantified phantom type parameters; by contrast, GDP relies on existentially-quantified
phantom names, and a rich, extensible set of combinators for building arguments about the named values.

Previous designs that use existentially-quantified phantom types include \emph{lazy state threads} \cite{launchbury1994lazy} and \emph{lightweight static capabilities} \cite{kiselyov2006lightweight}. The GDP approach explicitly separates two orthogonal concerns within these designs: the introduction of existentially-quantified type-level names, and the manipulation of proofs about those named values.

There are a variety of other approaches to checking the correctness of Haskell code. Liquid Haskell works with an SMT solver to verify
certain classes of properties about Haskell functions \cite{vazou2016liquid}. \texttt{hs-to-coq} converts Haskell code to
Coq code, allowing theorems to be proved within the Coq proof assistant \cite{spector2018total}. In both cases, the properties and proofs exist
outside of (or in parallel to) the existing Haskell code. In the GDP approach, properties and proofs are carried by
normal Haskell types and are checked by compilation.


\section{Summary}
Ghosts of Departed Proofs provides a novel approach to safe API design  that enables
a dialogue between the library and the user. By giving the user a vocabulary for expressing
safety arguments, GDP-style APIs avoid
the need for partial functions or optional returns. Using this approach, we are able to achieve
many of the benefits of dependent types and refinement types, while only requiring mild
and well-known extensions to Haskell 2010. 
You can try it with your own libraries, today!

\vspace{-0.4em}
\subsection*{Acknowledgments}
\vspace{-0.1em}
The author would like to thank Baldur Bl\"ondal, Hillel Wayne, Philipp Kant, Matt Parsons,
and the anonymous reviewers for
their helpful feedback on a draft of this paper, and Input Output HK for supporting
this work.

%\bibliographystyle{abbrvnat}
%\bibliography{gdp.bib}

\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abrams(2003)]{pitofsuccess}
B.~Abrams.
\newblock The pit of success.
\newblock
  \url{https://blogs.msdn.microsoft.com/brada/2003/10/02/the-pit-of-success/},
  2003.
\newblock Accessed: 2018-06-04.

\bibitem[Augustsson(1998)]{augustsson1998cayenne}
L.~Augustsson.
\newblock Cayennea language with dependent types.
\newblock In \emph{International School on Advanced Functional Programming},
  pages 240--267. Springer, 1998.

\bibitem[Bove and Dybjer(2009)]{bove2009dependent}
A.~Bove and P.~Dybjer.
\newblock Dependent types at work.
\newblock In \emph{Language engineering and rigorous software development},
  pages 57--99. Springer, 2009.

\bibitem[Breitner et~al.(2014)Breitner, Eisenberg, Peyton~Jones, and
  Weirich]{Breitner:2014:SZC:2692915.2628141}
J.~Breitner, R.~A. Eisenberg, S.~Peyton~Jones, and S.~Weirich.
\newblock Safe zero-cost coercions for haskell.
\newblock \emph{SIGPLAN Not.}, 49\penalty0 (9):\penalty0 189--202, Aug. 2014.
\newblock ISSN 0362-1340.
\newblock \doi{10.1145/2692915.2628141}.
\newblock URL \url{http://doi.acm.org/10.1145/2692915.2628141}.

\bibitem[Fluet and Pucella(2006)]{Fluet:2006:PTS:1180085.1180088}
M.~Fluet and R.~Pucella.
\newblock Phantom types and subtyping.
\newblock \emph{J. Funct. Program.}, 16\penalty0 (6):\penalty0 751--791, Nov.
  2006.
\newblock ISSN 0956-7968.
\newblock \doi{10.1017/S0956796806006046}.
\newblock URL \url{http://dx.doi.org/10.1017/S0956796806006046}.

\bibitem[Freeman and Pfenning(1991)]{freeman1991refinement}
T.~Freeman and F.~Pfenning.
\newblock Refinement types for {ML}.
\newblock In \emph{Proceedings of the ACM SIGPLAN 1991 Conference on
  Programming Language Design and Implementation}, PLDI '91, pages 268--277,
  New York, NY, USA, 1991. ACM.
\newblock ISBN 0-89791-428-7.
\newblock \doi{10.1145/113445.113468}.
\newblock URL \url{http://doi.acm.org/10.1145/113445.113468}.

\bibitem[Kiselyov and Shan(2004)]{kiselyov2004functional}
O.~Kiselyov and C.-c. Shan.
\newblock Functional pearl: implicit configurations--or, type classes reflect
  the values of types.
\newblock In \emph{Proceedings of the 2004 ACM SIGPLAN workshop on Haskell},
  pages 33--44. ACM, 2004.

\bibitem[Kiselyov and Shan(2007)]{kiselyov2006lightweight}
O.~Kiselyov and C.-c. Shan.
\newblock Lightweight static capabilities.
\newblock \emph{Electron. Notes Theor. Comput. Sci.}, 174\penalty0
  (7):\penalty0 79--104, June 2007.
\newblock ISSN 1571-0661.
\newblock \doi{10.1016/j.entcs.2006.10.039}.
\newblock URL \url{http://dx.doi.org/10.1016/j.entcs.2006.10.039}.

\bibitem[Launchbury and Peyton~Jones(1994)]{launchbury1994lazy}
J.~Launchbury and S.~L. Peyton~Jones.
\newblock Lazy functional state threads.
\newblock In \emph{ACM SIGPLAN Notices}, volume~29, pages 24--35. ACM, 1994.

\bibitem[Leavens et~al.(1999)Leavens, Baker, and Ruby]{leavens1999jml}
G.~T. Leavens, A.~L. Baker, and C.~Ruby.
\newblock {JML}: A notation for detailed design.
\newblock In \emph{Behavioral Specifications of Businesses and Systems}, pages
  175--188. Springer, 1999.

\bibitem[Leijen(2004)]{Leijen:2004:WPC:1017472.1017483}
D.~Leijen.
\newblock {wxHaskell}: A portable and concise {GUI} library for {Haskell}.
\newblock In \emph{Proceedings of the 2004 ACM SIGPLAN Workshop on Haskell},
  Haskell '04, pages 57--68, New York, NY, USA, 2004. ACM.
\newblock ISBN 1-58113-850-4.
\newblock \doi{10.1145/1017472.1017483}.
\newblock URL \url{http://doi.acm.org/10.1145/1017472.1017483}.


\bibitem[Leijen and Meijer(1999)]{Leijen:1999:DSE:1267936.1267945}
D.~Leijen and E.~Meijer.
\newblock Domain specific embedded compilers.
\newblock In \emph{Proceedings of the 2Nd Conference on Conference on
  Domain-Specific Languages - Volume 2}, DSL'99, pages 9--9, Berkeley, CA, USA,
  1999. USENIX Association.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=1267936.1267945}.

\vspace{6in}

\bibitem[Noonan(2018{\natexlab{a}})]{this}
M.~Noonan.
\newblock Ghosts of {D}eparted {P}roofs.
\newblock \url{http://www.github.com/matt-noonan/gdp-paper/},
  2018{\natexlab{a}}.
\newblock Accessed: 2018-06-03.

\bibitem[Noonan(2018{\natexlab{b}})]{this-hackage}
M.~Noonan.
\newblock The {gdp} library.
\newblock \url{http://hackage.haskell.org/package/gdp}, 2018{\natexlab{b}}.
\newblock Accessed: 2018-06-03.

\bibitem[Smullyan(1995)]{smullyan1995first}
R.~M. Smullyan.
\newblock \emph{First-order logic}.
\newblock Courier Corporation, 1995.




\bibitem[Spector-Zabusky et~al.(2018)Spector-Zabusky, Breitner, Rizkallah, and
  Weirich]{spector2018total}
A.~Spector-Zabusky, J.~Breitner, C.~Rizkallah, and S.~Weirich.
\newblock Total {Haskell} is reasonable {Coq}.
\newblock In \emph{Proceedings of the 7th ACM SIGPLAN International Conference
  on Certified Programs and Proofs}, pages 14--27. ACM, 2018.


\bibitem[Stolarek et~al.(2015)Stolarek, Peyton~Jones, and
  Eisenberg]{stolarek2015injective}
J.~Stolarek, S.~Peyton~Jones, and R.~A. Eisenberg.
\newblock Injective type families for {Haskell}.
\newblock In \emph{Proceedings of the 2015 ACM SIGPLAN Symposium on Haskell},
  Haskell '15, pages 118--128, New York, NY, USA, 2015. ACM.
\newblock ISBN 978-1-4503-3808-0.
\newblock \doi{10.1145/2804302.2804314}.
\newblock URL \url{http://doi.acm.org/10.1145/2804302.2804314}.

\bibitem[Timany et~al.(2017)Timany, Stefanesco, Krogh-Jespersen, and
  Birkedal]{timany2017logical}
A.~Timany, L.~Stefanesco, M.~Krogh-Jespersen, and L.~Birkedal.
\newblock A logical relation for monadic encapsulation of state: Proving
  contextual equivalences in the presence of {runST}.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 2\penalty0
  (POPL):\penalty0 64, 2017.

\bibitem[Vazou(2016)]{vazou2016liquid}
N.~Vazou.
\newblock \emph{Liquid Haskell: Haskell as a theorem prover}.
\newblock University of California, San Diego, 2016.

\bibitem[Volkov(2016)]{refined}
N.~Volkov.
\newblock Announcing the refinement types library.
\newblock \url{http://nikita-volkov.github.io/refined/}, 2016.
\newblock Accessed: 2018-05-30.

\bibitem[Vytiniotis et~al.(2011)Vytiniotis, Jones, Schrijvers, and
  Sulzmann]{vytiniotis2011outsidein}
D.~Vytiniotis, S.~P. Jones, T.~Schrijvers, and M.~Sulzmann.
\newblock {OutsideIn}(x): Modular type inference with local assumptions.
\newblock \emph{Journal of functional programming}, 21\penalty0 (4-5):\penalty0
  333--412, 2011.

\bibitem[Wadler(1990)]{wadler1990linear}
P.~Wadler.
\newblock Linear types can change the world!
\newblock In \emph{Programming Concepts and Methods}. North, 1990.

\end{thebibliography}
\end{document}
