\documentclass[format=sigplan, review=false, screen=true]{acmart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{fontspec}
\setmonofont[
  Contextuals={Alternate},
  Scale=0.9
]{Fira Code}
\makeatletter
\def\verbatim@nolig@list{}
\makeatother
\usepackage{microtype}

\usepackage{booktabs} % For formal tables
\usepackage{bussproofs}
\usepackage{graphics}
\usepackage{xcolor}

\usepackage{minted}
\usemintedstyle{friendly}
\setminted{
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    %bgcolor=lightgray,
    fontsize=\footnotesize,
    escapeinside=\#\#
    }

\usepackage{filecontents}

\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

\usepackage{listings}
\lstdefinestyle{mystyle}{
    %backgroundcolor=\color{backcolour},   
    commentstyle=\color{gray},
    keywordstyle=\color{blue},
    numberstyle=\tiny,
    stringstyle=\color{purple},
    basicstyle=\small\tt,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    %numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,
    frame=single,
    xleftmargin=1em,
    xrightmargin=1em,
    frame=shadowbox,
    rulesepcolor=\color{gray},
    tabsize=2
}
 
\lstset{style=mystyle,
  literate=
  {->} {$\to$} 2
  {<-} {$\leftarrow$} 2
  {=>} {$\Rightarrow$} 2
  {forall} {$\forall$} 1
  {exists} {$\exists$} 1
  {phi} {$\varphi$} 1
  {rho} {$\rho$} 1
  {kappa} {$\kappa$} 1
  {$nu$} {$\nu$} 1
  {$mu$} {$\mu$} 1
  {gamma} {$\gamma$} 1
  {subsetX} {$\subset$} 1
  {~>} {$\rightsquigarrow$} 2
  {<~>} {$\leftrightsquigarrow$} 3
  {elem} {$\in$} 1
}

\usepackage{cleveref}

% Metadata Information
\acmJournal{TWEB}
\acmVolume{9}
\acmNumber{4}
\acmArticle{39}
\acmYear{2018}
\acmMonth{3}
\copyrightyear{2018}
%\acmArticleSeq{9}

% Copyright
%\setcopyright{acmcopyright}
\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{0000001.0000001}

% Paper history
\received{June 2018}
%\received[revised]{March 2009}
%\received[accepted]{June 2009}

% Document starts
\begin{document}
% Title portion. Note the short title for running heads
\title[Ghosts of Departed Proofs]{Functional Pearl: Ghosts of Departed Proofs}

\author{Matt Noonan}
\orcid{1234-5678-9012-3456}
\affiliation{%
  \institution{Kataskeue LLC, Input Output HK}
%  \streetaddress{Esty St}
  \city{Ithaca}
  \state{NY}
  \postcode{14850}
  \country{USA}}
\email{mnoonan@kataskeue.com}


\begin{abstract}

  Library authors often are faced with a design choice: should a function with
  preconditions be implemented as a partial function, or by returning a failure
  condition on incorrect use? Neither option is ideal. Partial functions lead
  to frustrating run-time errors. Failure conditions must be checked
  at the use-site,
  placing an unfair tax on the users who have ensured that the function's
  preconditions were correctly met.
  
  In this paper, we introduce an API design concept called ``ghosts of departed
  proofs''. The key idea is that sophisticated preconditions can be
  encoded in Haskell's type system with no run-time overhead, by using proofs
  that inhabit phantom type parameters attached to \texttt{newtype} wrappers.
  The user expresses correctness arguments by constructing proofs to inhabit
  these phantom type parameters. Critically, this technique allows the
  library \emph{user} to decide when and how to validate that the APIs preconditions
  have been met.

  The ``ghosts of departed proofs'' approach to API design can acheive many of the benefits
  of dependent types and refinement types, while only requiring well-understood extensions to
  Haskell 2010. We demonstrate the utility of this approach
  through a series of case studies, showing how to enforce novel invariants for lists,
  maps, graphs, shared memory regions, and more.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
 \begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011074.10011099.10011692</concept_id>
<concept_desc>Software and its engineering~Formal software verification</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011006.10011008.10011024.10011025</concept_id>
<concept_desc>Software and its engineering~Polymorphism</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10011007.10011074.10011075.10011078</concept_id>
<concept_desc>Software and its engineering~Software design tradeoffs</concept_desc>
<concept_significance>100</concept_significance>
</concept>
<concept>
<concept_id>10003752.10003790.10002990</concept_id>
<concept_desc>Theory of computation~Logic and verification</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Formal software verification}
\ccsdesc[300]{Software and its engineering~Polymorphism}
\ccsdesc[100]{Software and its engineering~Software design tradeoffs}
\ccsdesc[300]{Theory of computation~Logic and verification}
%
% End generated code
%


\keywords{API design, software engineering, formal methods, higher-rank types}

\maketitle

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{M. Noonan}

%\input{samplebody-journals}

\section{Introduction}
\begin{quote}
  [Rico Mariani] admonished us to think about how we can build platforms that lead developers to write great, high performance code such that developers just fall into doing the ``right thing''. That concept really resonated with me. It is the key point of good API design. We should build APIs that steer and point developers in the right direction.
  
  \hfill --- Brad Abrams \cite{pitofsuccess}
\end{quote}

What is the purpose of a powerful type system? One practical perspective is
that a type system provides a mechanism for enforcing program
invariants at compile time. The desire to encode increasingly
sophisticated program invariants has led to a vast expanse of research
on more complex type systems, including dependent types \cite{augustsson1998cayenne,bove2009dependent}, refinement types \cite{freeman1991refinement}, linear
types \cite{wadler1990linear}, and many more. But despite this menagerie of powerful
type systems, workaday Haskell programmers have already been able to encode
surprisingly sophisticated invariants using nothing more than a
few well-understood extensions to the Damas-Hindley-Milner type system.

An early success story is the \texttt{ST} monad, which allows pure
computations to make use of local, mutable state. A phantom type parameter
and a clever use of rank-2 types in the \texttt{ST} monad's API allows the
compiler to ensure that the local mutable state is invisible from the outside,
and hence the resulting computation really \emph{is} pure. As we will see, this
trick is just the tip of a rather large iceberg.

In this paper, we will take the perspective of a library author, writing in
Haskell 2010 (plus a few battle-tested language extensions). As a library
author, our goal will be to design \emph{safe} APIs that are also \emph{ergonomic}
for the end user. ``Safe'' means that we want to prevent the user from causing a
run-time error. ``Ergonomic'' means that we do not want the correct use of our
API to put an undue burden on the user.

\begin{filecontents*}{idioms.hs}
-- Unsafe API using non-total functions.
head :: [a] -> a
head xs = case xs of
  (x:_) -> x
  []    -> error "empty list!"

endpts = do
  putStrLn "Enter a non-empty list of integers:"  
  xs <- readLn
  return (head xs, head $ reverse xs)

--------------------------------------------------------
-- Returning Maybe / Optional values. Safe, but requires
-- the caller to pattern-match on the Maybe at every use,
-- even when the list is known to be non-empty. Frustrated
-- users cannot be blamed for using `fromJust`!
safeHead :: [a] -> Maybe a
safeHead xs = case xs of
  (x:_) -> Just x
  []    -> Nothing

safeEndpts = do
  putStrLn "Enter a non-empty list of integers:"  
  xs <- readLn
  case safeHead xs of
    Just x -> return (x, fromJust (safeHead $ reverse xs))
    _      -> safeEndpts

-------------------------------------------------------
-- "Ghosts of Departed Proofs". Safe. Does not return
-- an optional value; preconditions are checked early
-- and carried by "ghosts" (specialized phantom types).
rev_cons :: IsCons xs -> Proof (IsCons (Reverse xs))

gdpHead :: (a ~~ xs ::: IsCons xs) -> a
gdpHead xs = head (the xs)

gdpEndpts = do
  putStrLn "Enter a non-empty list of integers:"  
  xs <- readLn
  name xs $ \xs -> case classify xs of
    Right is_cons -> let ok = is_cons |$ rev_cons in
      return (gdpHead xs, gdpHead (gdpRev xs ...ok))
    Left  _  -> gdpEndpts
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{idioms.hs}
  \caption{Idioms for implementing the
    \texttt{head} function, along with usage examples.
    The \texttt{gdpHead} function can only be invoked in
    a context where the user has already proven that the
    list is non-empty, combining the simplicity of the
    first example with the safety of the second. \texttt{rev\_cons}
    is a proof combinator exported by the library to help the
    user prove that the reverse of a non-empty list is still non-empty. See
  \cref{sizing,size-evidence} for details.}
\end{figure}

\subsection{Common idioms for handling pre-conditions}

No matter the language, a programmer often has to write functions
that place constraints on their input. For example, the venerable
\texttt{head} function for extracting the first element of a list
asks its users to only hand it a non-empty list to operate on.
Now put yourself in the shoes of \texttt{head}'s author: how can
you ensure that \texttt{head} will be used properly? Let us recount
a variety of strategies used in the wild.

\paragraph{Run-time failure on bad inputs.}
Often the simplest approach is
  to have a function simply fail on malformed inputs. The failure can
  be an immediate run-time error (as in Haskell's \texttt{head}), or result in
  undefined behavior (as in \texttt{C++}'s \texttt{std::vector<T>::front()}).
  
\paragraph{Returning a dummy value.}
  To avoid run-time errors, some APIs may have a ``dummy value''
  for indicating the result of a failed operation. For example, Common Lisp's
  \texttt{car} and \texttt{golang}'s \texttt{Front()} both return \texttt{nil}
  when passed an empty list. The caller must explicitly check for this dummy
  value. Other contortions may be needed if the container is also allowed to
  hold \texttt{nil}, to disambiguate between ``the input list was empty'' and
  ``\texttt{nil} is the first element of this list''.

\paragraph{Returning a value with an option type.}
  A similar ``dummy value'' strategy
  for languages with stricter typing discipline is to use an ``option type'' such
  as Haskell's \texttt{Maybe} or Scala's \texttt{Option}. A value of type \texttt{Maybe a}
  cannot be used where a value of type \texttt{a} was expected, so the user must
  explicitly pattern match on the optional value to extract the result and handle the
  error case. This approach may lead to frustration when the user believes that the
  error case is not possible.
  
\paragraph{Modifying input types to exclude bad inputs.}
Finally, the API designer may select more restrictive types for the inputs in order
to make the function total. For example, some Haskell libraries make use of the
\texttt{NonEmpty} type for lists that contain at least one element. The \texttt{head}
function then becomes total. Rather than handling a failure case, the user only
needs to prove that their list is non-empty by making use of the smart constructor
\texttt{nonEmpty :: [a] -> Maybe (NonEmpty a)}.

\subsection{Ghosts of Departed Proofs}
The ``return-an-optional-value'' idiom is well-known in the functional programming world.
The author of a library function that returns \texttt{Maybe a} can certainly sleep well
at night, content in the knowledge that their function will never cause a run-time error.

But what about the \emph{users} of that library? Has the library author helped the user stay
on a virtuous path, or have they led the user into temptation?

In fact, the author of the library has just pushed extra responsiblity onto the user.
Every time the user applies a function that uses the optional-return idiom, they are obliged
to test the return value and handle the error case. This remains true, even if the user
has \emph{correctly} ensured that the function's preconditions have been met! The library
author sleeps well while even the most cautious users toil against those impossible error cases.

No wonder so many well-meaning users reach for unsafe functions like \texttt{fromJust}!
They have already proved (to their own satisfaction) that the function is being used properly, so they rightly
feel justified in ignoring the error case entirely. But now we see how the user has been led into
a pit of despair: they have ended up with a program that is \emph{exactly as fragile} as one where the library
author had used the run-time-failure idiom!\footnote{In fact, things are slightly \emph{worse}: we have also introduced a little
bit of extra allocation and indirection for creating and unpacking the return value in the non-error case.}
Even if the user has mentally constructed a proof that this specific use of \texttt{fromJust} is safe \emph{right now},
who can say what will
happen as the software changes over time? Without tooling to ensure that the user's proof \emph{remains} valid,
the software is left in a brittle state.


For example, a recent snapshot of \texttt{hackage}
reveals over 2000 instances where the partial function
\texttt{fromJust} is applied to the
result of \texttt{Data.Map}'s \texttt{lookup}. Each one
records the moment a programmer fell into a pit of despair:
they have somehow reasoned that a certain key must be
present in the map, but were given no mechanism for
\emph{communicating} that reasoning to the
\texttt{Data.Map} API. To escape the pit, they made the
pragmatic---but unsafe---decision to introduce partiality.
We can do better than this.


\subsection{The structure of this paper}
In this paper, we will use a series of case studies to show how library authors can create
APIs that are both \emph{safe} and \emph{ergonomic}. By \emph{safe}, we mean that the
user cannot cause a run-time error or undefined behavior when using the API. Incorrect uses
will become compile-time errors. By \emph{ergonomic}, we mean that the API is straightforward
enough that the user is not tempted to subvert the library's safety guarantees by using
unsafe functions. Crucially, we want the user to be able to \emph{communicate} their
informal proofs to the library. If the user believes that a precondition has been met,
they should be able to explain \emph{why} to the library!

The GDP technique is simple to implement. Each case study includes example library code, along with usage examples. Although the examples in this paper are largely self-contained, a
project suitable for experimentation in \texttt{GHCi} is also available in this paper's \texttt{git} repository \cite{this}.


\subsection{Summary of the critical features}
In the following sections, we will investigate several case studies that
demonstrate how the GDP approach can help create libraries for common
software engineering tasks, while ensuring that the library is both
safe and ergonomic for the end-user. The critical features of the GDP
approach are:

\begin{itemize}
\item Use \texttt{newtype} wrapper around the types of interest, equipped with a phantom
  type parameter. The \texttt{newtype} wrapper is erased during compilation, leaving no run-time cost.
\item Use the phantom type parameter to carry ``ghosts of departed proofs'': compile-time evidence
  that the ********
\item API functions for creating wrapped values. This may mean creating certain values with
  known properties (\texttt{nil} in \cref{size-evidence}), classifying a value into
  mutually disjoint subtypes (\texttt{classify} in \cref{size-evidence})
  or introducing existentially-quantified properties (\texttt{sizing} in \cref{sizing}).
\item Proof combinators for manipulating the form of the ghost proofs (\texttt{zero\_neutral}
  and \texttt{add\_commutes} in \cref{size-evidence}). The proof combinators should be
  implemented as coercions that are erased during compilation, to avoid any run-time cost.
\end{itemize}

The specific benefits of the GDP approach to library design are:
\begin{itemize}
\item Only mild extensions to a Damas-Hindley-Milner type system
  ({\it e.g.} rank-2 types, a module system, phantom types) are required to
  implement the technique.
\item Propositions are encoded in the phantom types, so the
  \emph{proofs} are written in the normal value-level language.
  No type-level computation is required.
\item Proofs are explicit, not implicit as in Liquid Haskell. There is
  no reliance on third-party solvers that may fail to complete a check,
  or may vary in their behavior from platform to platform.
\item Proofs are concrete entities within the host language, and can
  be analyzed or audited independently. Tooling for the host language
  can be used within the proof language.
\end{itemize}

\section{Case Study \#1: Sorted lists}

Surely every programmer has, at some time, had to work with collections of
lists that were required to be sorted in one way or another. Within some
context, the programmer may need to ensure that certain invariants hold, such
as ``all of these lists have been sorted by the same comparator''. For a concrete
example, consider these \texttt{sortBy} and \texttt{mergeBy} functions:
\begin{minted}{haskell}
sortBy  :: (a -> a -> Ordering) -> [a] -> [a]

-- Usage constraint: in `mergeBy comp xs ys`, the
-- input lists `xs` and `ys` should also be sorted
-- by the same comparator `comp`.
mergeBy :: (a -> a -> Ordering) -> [a] -> [a] -> [a]
mergeBy comp xs ys = go xs ys
  where
    go []  ys' = ys'
    go xs' []  = xs'
    go (x:xs') (y:ys') = case comp x y of
      GT -> y : go (x:xs') ys'
      _  -> x : go xs' (y:ys')
\end{minted}
This efficient $O(n+m)$ implementation of \texttt{mergeBy} is easy to write,
but puts the onus on the caller of \texttt{mergeBy} to ensure that the
input lists have been sorted by the same comparator. If the user accidentally
fails to sort the two inputs in the same way, the result of \texttt{mergeBy}
will silently be nonsense, possibly leading to a difficult-to-diagnose bug.

It would be possible to implement a version of \texttt{mergeBy} that
carefully inspected the inputs \texttt{xs} and \texttt{ys} as it
proceeded, and only produced a result if the inputs met the sorting
requirement. But this would impose a runtime cost on every use of
\texttt{mergeBy}, increase the complexity of its implementation,
and change the result type to \texttt{Maybe [a]}. And then what?
Most users of \texttt{mergeBy} would argue to themselves ``This is
absurd! I already know the input lists were sorted properly. This
function will never result in \texttt{Nothing}.'' It would be hard
to blame the user when they reach for an  unsafe function like
\texttt{fromJust}.

Clearly, there is little benefit for anybody in the above scenario. The
library author is inconvenienced by the increased implementation complexity.
The user is inconvenienced by the decreased performance and the need to
pattern match on the result, even when they believe they already know the
outcome of that match. No wonder that the status quo is to just write
a stern warning in the documentation, admonishing
any user who tries to \texttt{mergeBy} what they didn't \texttt{sortBy}.

But what if the user really \emph{does} have proof that the input lists have
been sorted properly? Can we devise a mechanism that allows the user to communicate
this proof to \texttt{mergeBy}?

\subsection{Conjuring a name}
The first challenge that we meet is how to reflect the notion of two  comparators
being ``the same''. In a language that supports equality tests on functions,
you could imagine a solution where the \texttt{sortBy} function returns a sorted
list along with a reference to the comparator that was used; \texttt{mergeBy} could
then check that the comparators matched. But this has a run-time cost for carrying
around the comparator references, and it still would require \texttt{mergeBy} to
return \texttt{Nothing} if it was given bogus arguments.

Following the GDP approach, we can introduce a proposition ``is named \texttt{name}'',
represented by a \texttt{newtype} wrapper with phantom type parameter \texttt{name}.
In code, we will write this proposition as \verb|a ~~ n|, to be read as
``values of type \texttt{a} with name \texttt{n}''. A simple module for named values
can be found in \cref{name-module}; the only property that we encode is
``any value can be given a name'', represented by the exported \texttt{name} function.
In \cref{case4}, we will extend this module with extra functionality in support of
custom, library-defined names.


\begin{filecontents*}{named.hs}
module Named (Named, type (~~), name) where

import The

newtype Named name a = Named a
instance The (Named name a) a
type a ~~ name = Named name a

name :: a -> (forall name. (a ~~ name) -> t) -> t
name x k = k (coerce x)
\end{filecontents*}

\begin{filecontents*}{ordered.hs}
module Sorted (Named, SortedBy, sortBy, mergeBy) where   

import The
import Named

import           Data.Coerce
import qualified Data.List       as L
import qualified Data.List.Utils as U

newtype SortedBy o a = SortedBy a
instance The (SortedBy o a) a
  
sortBy :: ((a -> a -> Ordering) ~~ comp)
       -> [a]
       -> SortedBy comp [a]
sortBy comp xs = coerce (L.sortBy (the comp) xs)

mergeBy :: ((a -> a -> Ordering) ~~ comp)
        -> SortedBy comp [a]
        -> SortedBy comp [a]
        -> SortedBy comp [a]
mergeBy comp xs ys =
  coerce (U.mergeBy (the comp) (the xs) (the ys))        
\end{filecontents*}

\begin{filecontents*}{usageO.hs}
import Sorted
import Named
main = do
  xs <- readLn :: IO Int
  ys <- readLn
  name (comparing Down) $ \gt -> do
    let xs' = sortBy gt xs
        ys' = sortBy gt ys
    print (the xs', the ys', the (mergeBy gt xs' ys'))
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{ordered.hs}
  \caption{A module for working with lists that have been sorted by an arbitrary
    comparator. The refinement \texttt{SortedBy comp} is used to denote values that
    have been sorted by the comparator named \texttt{comp}.}
\end{figure}


\begin{figure}
  \inputminted{haskell}{named.hs}
  \caption{A module for attaching ghostly names to values.\label{name-module}}
\end{figure}

\begin{figure}
  \inputminted{haskell}{usageO.hs}
  \caption{Usage example}
  \end{figure}
Clients of the library are somewhat more restricted, in the sense that they cannot create a
value of type \texttt{SortedBy comp t} without going through the library's public API.

\begin{minted}[frame=none]{haskell}
minimum_O1 :: SortedBy comp [a] -> Maybe a
minimum_O1 xs = case (the xs) of
    []    -> Nothing
    (x:_) -> Just x
\end{minted}

\subsection{The danger of naming a ghost}

Finally, for the user to be able to \emph{use} this library, there must be a way for
them to create \texttt{Named} values from normal values. The library must export a
function similar to this:

\begin{minted}[frame=none]{haskell}
name :: a -> (forall name. (a ~~ name) -> t) -> t
name x k = k (coerce x)
\end{minted}

This function is quite similar to \texttt{sizing} from the previous section, and the rank-2
type gives it a bit of an ominous feel. You might wonder: why not just have a function
with a simple type like this?

\begin{minted}[frame=none]{haskell}
any_name :: a -> (a ~~ name)
any_name = coerce
\end{minted}

At its root, the question is really about \emph{who gets to choose} what \texttt{name} will be.
In the signature of \texttt{any\_name}, the \emph{caller} gets to select the types \texttt{a}
and \texttt{name}. In particular, they can attach any name they would like!

If that still does not sound so bad, consider this code:

\begin{minted}[frame=none]{haskell}
data Simon
  
up, down :: (Int -> Int -> Ordering) ~~ Simon
up   = any_name compare
down = any_name (comparing Down)

list1 = sortBy up   [1,2,3]
list2 = sortBy down [1,2,3]

merged = the (mergeBy up list1 list2) :: [Int]
-- [1,2,3,3,2,1]
\end{minted}
\noindent
The user has decided to name two different functions \texttt{Simon}, subverting the
guarantees offered by the API of the \texttt{Sorted} module. It is dangerous to
name a ghost!

Now compare this to the analogous program, using \texttt{name} instead of \texttt{any\_name}:
\begin{minted}[frame=none]{haskell}
name compare $ \up ->
  name (comparing Down) $ \down ->
    let list1 = sortBy up   [1,2,3]
        list2 = sortBy down [1,2,3]
    in the (mergeBy up list1 list2)
\end{minted}
\noindent
Attemptig to compile this program results in a type error:

\begin{lstlisting}
  â€¢ Couldn't match type "name1" with "name"
        ...
    Expected type: SortedBy name [Integer]
      Actual type: SortedBy name1 [Integer]
\end{lstlisting}
\noindent
What is the critical difference between these two examples? In the first, a user is
allowed to \emph{create} a named value by fiat. In the second, the user is only allowed to \emph{consume} a named value, by
providing a polymorphic function that can work with \emph{any} named value. The library's API provides
a helper function---in this case, \texttt{name}---for applying the consumer to a normal, unnamed value.
In practice, it is as if the
library has a secret supply of names, and selects one to use in a manner that is not
predictable (or even inspectable!) to the user.

A general rule of thumb for library authors is:
\emph{a ghost should not appear in the return type,  unless it also appears in an argument's type}. This simple rule ensures that
the user of the library will not be allowed to materialize ghosts out of thin air.

\section{Case Study \#2: State threads with sharing}
The trick for using rank-2 types to conjure names outside of the user's control was
inspired by the \texttt{ST} monad and its rank-2 \texttt{runST :: (forall s. ST s a) -> a}
function \cite{launchbury1994lazy}. In this brief case-study, we elaborate the connection
between the \texttt{ST} monad and GDP-style names. The new perspective suggests novel
extensions to the \texttt{ST} API.
In \Cref{st-api} we recall the basic \texttt{ST} API \cite{launchbury1994lazy}, writing \texttt{St} to
disambiguate our version from the existing type in \texttt{Control.Monad.ST}.

In their safety analysis of the \texttt{ST} monad, Timany \textit{et al.} proposed to think of the \texttt{s} parameter as
representing a name attached to a region of the heap \cite{timany2017logical}.
Informally, we can think of \texttt{ST s a} as representing a \texttt{State} monad over
\emph{named stores}, like so:
\begin{minted}[frame=none]{haskell}
data Store = Store
newtype Region s = Region Defn

type St s = State (Store ~~ Region s)

runSt :: (forall s. St s a) -> a
runSt action = defining Store (evalState action)
\end{minted}

The notion of treating the \texttt{ST} monad's phantom type as a region name immediately leads to ideas for
other primitives. Once we can name regions, what's to stop us from creating more detailed names to describe
the minute contours of those regions? For example, let us add a binary type constructor $\cap$ so that \texttt{s $\cap$ s'}
names the region at the intersection of \texttt{s} and \texttt{s'}. We are quickly led to an API similar to \Cref{st-sharing-api},
where individual sub-computations can, at their discretion, share mutable reference cells with other sub-computations.

\begin{filecontents*}{st1.hs}
runSt    :: (forall s. St s a) -> a

newRef   :: a -> St s (a #$\in$# Region s)
readRef  :: (a #$\in$# Region s) -> St s a
writeRef :: (a #$\in$# Region s) -> a -> St s ()
\end{filecontents*}

\begin{filecontents*}{st2.hs}
runSt2 :: (forall s s'. St (s #$\cap$# s') a) -> a

liftL :: St s a -> St (s #$\cap$# s') a
liftR :: St s' a -> St (s #$\cap$# s') a

share :: (a #$\in$# Region s) -> St s (a #$\in$# Region (s #$\cap$# s'))

use  :: (a #$\in$# Region (s #$\cap$# s')) -> (a #$\in$# Region s)
symm :: (a #$\in$# Region (s #$\cap$# s')) -> (a #$\in$# Region (s' #$\cap$# s))
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{st1.hs}
  \caption{The standard ``state thread'' API. We write \texttt{a $\in$ Region s} to
    denote a reference cell of type \texttt{a} in the memory region called \texttt{s}.
    In \texttt{Control.Monad.ST}, we would write \texttt{a $\in$ Region s} as
    \texttt{STRef s a}.\label{st-api}}
\end{figure}

\begin{figure}
  \inputminted{haskell}{st2.hs}
  \caption{Extending the state thread API with shared references.\label{st-sharing-api}}
\end{figure}
Critically, the ``shared reference'' API only allows the
original state thread to move a reference into the shared region.

In effect, \texttt{runSt2} lets the user run a computation that makes use of
two partially-overlapping memory regions. Within that computation, the user
can run sub-computations bound to one or the other memory region. Furthermore,
a sub-computations can move any variable that it owns into the common overlap
via \texttt{share}. An example is shown in \Cref{st-example}, where one sub-computation
creates both a shared and a private reference cell. A second sub-computation has
access to the shared cell. Yet even though the private reference is in scope during
the second sub-computation, any attempt to access it results in a compilation error.

\begin{filecontents*}{st.hs}
stSharingDemo :: Bool
stSharingDemo = runSt2 $ do
  -- In the "left" memory region, create and return
  -- two references; one shared, and one not shared.
  (secret, ref) <- liftL $ do
      unshared <- newRef 42
      shared   <- share =<< newRef 17
      return (unshared, shared)
  -- In the "right" memory region, mutate the shared
  -- reference. If we attempt to access the non-shared
  -- reference here, the program will not compile.
  liftR $ do
      let mine = use (symm ref)
      x <- readRef mine
      writeRef mine (x + 1)
  -- Back in the "left" memory region, verify that the
  -- unshared reference still holds its original value.
  liftL $ do
      check <- readRef secret
      return (check == 42)
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{st.hs}
  \caption{An \texttt{ST}-style pure computation using local mutable
    references. By introducing two named regions at once, we can extend
    the \texttt{ST} API with new capabilities such as shared references.\label{st-example}}
\end{figure}

\section{Case Study \#3: \texttt{Maybe}-free lookup in containers}

In this section, we will investigate the pit of despair that leads so
many developers to follow a map lookup by \texttt{fromJust}.

It is instructive to compare the two functions \texttt{lookup :: k -> Map k v -> Maybe v}
and \texttt{lookup :: (k $\in$ ks) -> JMap ks k v -> v}. We do not intend to claim that
one of these is better than the other. Instead, the claim is that these two functions
\emph{reflect different states of the user's knowledge}.

If the user legitimately does not know whether or not a key is present, then the
\texttt{Maybe}-returning \texttt{lookup} is called for. The user's incomplete knowledge
about the result of the operation is exactly reflected in the return type, so they will
not feel inconvenienced by the need to handle both the \texttt{Just v} (key present)
and \texttt{Nothing} (key absent) cases.

On the other hand, if the user already believes the key should be present based on some
external evidence, then they will be happier writing a program that does not need to handle
the impossible missing-key state. But they must demonstrate that evidence to the library
somehow!  ****

\subsection{Application: well-formed adjacency lists}

The power of this method becomes more apparent when considering maps where
the values are expected to reference the keys in some way. For example, a
simple representation for directed graphs with vertex type \texttt{v} is:
\begin{minted}[frame=none]{haskell}
type Digraph v = Map v [v]
\end{minted}
mapping each vertex to its list of immediate neighbors. Well-formed \texttt{Digraph}s
should satisfy the property that every vertex referenced in any neighbor list is also
a valid key in the adjacency map.

Traditionally, graph APIs using adjacency representations require well-formed
graphs, but make it the user's responsiblity to ensure well-formedness. For example,
the \texttt{Data.Graph} API from \texttt{containers} has a graph constructor that
will silently discard edges with targets that do not appear in the node list.
%%In \texttt{C++}, \texttt{boost::adjacency\_list} has an \texttt{add\_edge} function
%%that returns a \texttt{bool}. This value must be

The GDP-style structure of the \texttt{justified-containers} API makes it easy to
translate the notion of ``well-formed adjacency list'' into an invariant that can
be checked by the compiler. We simply write what we mean: a well-formed adjacency
list is a map from vertices to a list of vertices that are keys of that same map.
In other words:
\begin{minted}[frame=none]{haskell}
type Digraph vs v = JMap vs v [v #$\in$# vs]
\end{minted}

\begin{filecontents*}{justified.hs}
newtype JMap ks k v = JMap (Map k v) deriving Functor
newtype k #$\in$# ks = Element k

instance The (JMap ks k v) (Map k v)
instance The (k #$\in$# ks) k

member   :: k -> JMap ks k v -> Maybe (k #$\in$# ks)
lookup   :: (k #$\in$# ks) -> JMap ks k v -> v
reinsert :: (k #$\in$# ks) -> v -> JMap ks k v -> JMap ks k v
withMap  :: Map k v -> (forall ks. JMap ks k v -> t) -> t
\end{filecontents*}

\begin{filecontents*}{justified-usage.hs}
test = Map.fromList [ (1, "Hello"), (2, "world!") ]

name test $ \table -> name 1 $ \k ->
  case member k table of
 
    Nothing  -> putStrLn "Missing key!"
  
    Just key -> do
      let table'  = reinsert key "Howdy" table
          table'' = fmap (map upper) table
      putStrLn ("Value in map 1: " ++ lookup key table)
      putStrLn ("Value in map 2: " ++ lookup key table')
      putStrLn ("Value in map 3: " ++ lookup key table'')
-- Output:
--   Value in map 1: Hello
--   Value in map 2: Howdy
--   Value in map 3: HELLO
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{justified.hs}
  \caption{A fragment of the API from \texttt{justified-containers}.
    The GDP-style predicates \texttt{k $\in$ ks} and \texttt{JMap ks k v} are used to represent
    ``a value of type \texttt{k} belonging to the set \texttt{ks}'' and ``a map with key set \texttt{ks}'',
    respectively. \label{justified-api}}
\end{figure}
\begin{figure}
  \inputminted{haskell}{justified-usage.hs}
  \caption{A usage example for the API in \Cref{justified-api}. The \texttt{member} function is used
    to check if a key is present in \texttt{table}; within the scope of the \texttt{Just} case, \texttt{key}
    carries a phantom proof of its presence in \texttt{table}. The same phantom proof can also be used as evidence that
    \texttt{key} is present certain other maps as well, such as \texttt{table'} (\texttt{table} with a
    value changed) and \texttt{table''} (\texttt{table} modified by \texttt{fmap}).} 
\end{figure}

%%%% Maybe not so relevant?
%% \subsection{Application: Faster lookup}
%% Containers such as \texttt{Data.Map} that make use of a total ordering on the keys
%% generally are designed to allow for lookup in $O(\log n)$ comparisons, where $n$ is the
%% total number of keys in the map. If the comparator used for the keys is unusually
%% expensive, ***. For this situation, \texttt{Data.Map} module offers ``indexed'' versions
%% of many operations, identifying keys by their order in the map rather than by their actual
%% value. Lookup with a key's index then requires $O(\log n)$ \emph{integer} comparisons.

%% Although \texttt{justified-containers} defines a simple \texttt{newtype} wrapper for
%% the key-plus-phantom-proof type, more interesting information about the location of
%% the key within the corresponding data structure can sometimes be attached.

%% For example, imagine a simple binary search tree backed by a vector of key-value pairs.
%% As in the previous example, we will give the \texttt{BST} type a phantom parameter that
%% represents the set of valid keys present in the tree. But instead of wrapping the key
%% type directly, we will use an index-plus-phantom-proof representation for keys.


\subsection{Changing the key set}\label{changing-keys}
But what about maps that are related, yet do not have exactly the same key sets?
For example, the \texttt{insert} function will usually modify the key set of a map,
yet we know that if a key was present in the original map, it must still be present
in the expanded map. Imagine you were a user, in possession of a key and a proof
that it is present in the original map. It would be quite frustrating if  we were
unable to use that same key freely in the expanded map!
The library author, anticipating this need, should provide a proof combinator for
turning a proof that \texttt{k} is a valid key of \texttt{m} into a proof that
\texttt{k} is also a valid key of \texttt{insert k' v m} for any \texttt{k'}.

To support this use-case, \texttt{justified-containers} provides the functions
\texttt{inserting} and \texttt{deleting}, with these signatures:
\begin{minted}{haskell}
deleting  :: Ord k => k #$\in$# ks -> JMap ks k v
          -> (forall ks'. JMap ks' k v
                      -> (k #$\in$# ks' -> k #$\in$# ks)
                      -> t)
          -> t

inserting :: Ord k => k -> v -> JMap ks k v
          -> (forall ks'. JMap ks' k v
                       -> (k #$\in$# ks -> k #$\in$# ks')
                       -> k #$\in$# ks
                       -> t)
          -> t
\end{minted}
Since these functions each result in maps with new key sets, we must
introduce the ghost of these new key sets inside another \texttt{forall}.
But what are the other parameters for? In the case of \texttt{inserting},
the computation has access to:
\begin{enumerate}
\item The updated map, of type \texttt{JMap ks' k v}. The phantom type \texttt{ks'}
  represents the key set \texttt{ks}, updated with the newly-inserted key.
\item A function that represents the inclusion of \texttt{ks} into \texttt{ks'}.
  The user can apply this function to convert a proof that a certain key belonged to the
  old map (a value of type \texttt{k $\in$ ks}) into a proof that the key also belongs to the new map (a value of type \texttt{k $\in$ ks'}).
\item Evidence that the new key is present in the new key set.
\end{enumerate}

Similarly, the continuation for \texttt{deleting} has access to both the updated map
and a representation of the inclusion of keys from the new map into the old map.

The library author must do a bit of a balancing act here. It is important to provide
the user with a supply of evidence and proof combinators so that they are able to
express the fact that the API is being used properly, but it is not always clear how
much information should be added. For example, the user may well want to argue that a
every key \emph{other} than the deleted one is still present in a map modified by
\texttt{deleting}, but the API provides no straightforward way to do this.

\begin{filecontents*}{defn.hs}
-- Continuing module Named:
data Defn = Defn -- Type exported, constructor hidden.

type Defining f = (Coercible f Defn, Coercible Defn f)

defn :: Defining f => a -> (a ~~ f)
defn = coerce
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{defn.hs}
  \caption{Extending the \texttt{Named} module with
    an API that allows library authors to introduce
    special names by fiat. The \texttt{Defining} typeclass
    leverages \texttt{GHC}'s magic \texttt{Coercible} class \cite{Breitner:2014:SZC:2692915.2628141},
    ensuring that \texttt{defn} is available a library's author but not
    to the library's users. \Cref{lemma-demo} provides a usage example.}
\end{figure}

\begin{filecontents*}{delete1.hs}
newtype Insert k v m = Insert Defn
newtype Delete k m   = Delete Defn

insert :: Ord k
       => (k ~~ k)
       -> (v ~~ v)
       -> (Map k v ~~ m)
       -> (Map k v ~~ Insert k v m)
insert k v m =
    defn (Map.insert (bare k) (bare v) (bare m))
       
delete :: Ord k
       => (k ~~ k ::: k #$\in$# Keys m)
       -> (Map k v ~~ m)
       -> (Map k v ~~ Delete k m)
delete k m = defn (Map.delete (bare k) (bare m))
\end{filecontents*}
\begin{filecontents*}{delete2.hs}
subset_elts :: (a #$\subseteq$# b) -> (x #$\in$# a) -> Proof (x #$\in$# b)
subset_elts _ = sorry

insert_supset :: Proof (Keys m #$\subseteq$# Keys (Insert k v m))
insert_supset = sorry

key_is_present :: Proof (k #$\in$# Insert k v m)
key_is_present = sorry

delete_subset :: Proof (Keys (Delete k m) #$\subseteq$# Keys m)
delete_subset = sorry

key_is_missing  :: (k' #$\in$# Keys m)
                -> Not (k == k')
                -> Proof (k' #$\in$# Keys (Delete k m))
key_is_missing _ _ = sorry                
\end{filecontents*}

\begin{figure*}
  \begin{minipage}{0.49\textwidth}
    \inputminted{haskell}{delete1.hs}
  \end{minipage}
  \begin{minipage}{0.49\textwidth}
    \inputminted{haskell}{delete2.hs}
  \end{minipage}
  \caption{Separation of lemmas from API functions. The functions
    \texttt{insert} and \texttt{delete} replace the more complex
    \texttt{inserting} and \texttt{deleting} from \texttt{justified-containers}.
    These functions are also more straightforward for the user, since they do
    not involve the rank-2 continuations seen in \texttt{inserting} and \texttt{deleting} (compare \cref{changing-keys}).
    The library author can add any number of useful lemmas without burdening the
    user. General-purpose lemmas such as \texttt{subset\_elts} can be bundled
    together into modules and re-used across libraries.\label{lemma-demo}}
\end{figure*}

\section{Case Study \#4: Arbitrary invariants}

As a final case study, we investigate how the GDP technique can be used to
create APIs that can enforce arbitrary conditions on functions. ****

The types used to enforce non-emptiness of the input to \texttt{gdpHead} in
\Cref{idioms} is similar to the \texttt{Refined} type from the \texttt{refinement}***
package. ***

In this case study, let's consider what kind of APIs we could write if we separate
type-level names for each value from the constraints we want to place on those values.
For example, let us return to the \texttt{head} function. We want ensure that the
user only calls \texttt{head} on a list \texttt{xs} with outer constructor \texttt{(:)} (``cons''). We can now write, very explicitly, the requirement that the library asks of the user
when using \texttt{head}: the parameter, called \texttt{xs}, must have outermost constructor
\texttt{(:)}. So we simply introduce a predicate \texttt{IsCons}, and write down the definition:
\begin{minted}{haskell}
newtype IsCons xs = IsCons Defn
head :: ([a] ~~ xs ::: IsCons xs) -> a
\end{minted}
Here, the \texttt{:::} should be read ``such that''; altogether, the phrase 
\verb|(a ~~ n ::: p)| can be read ``a value of type \texttt{a}, named \texttt{n}, such
that condition \texttt{p} holds.''

The primary difference between names and other predicates is that a name, by construction,
is permanently attached to a value, while predicates can be attached or detached from values
more flexibly. Of course, it wouldn't do if a user could freely attach \emph{any} predicate to a value. Instead, we want to ***

\begin{filecontents*}{pred.hs}
-- Type exported, constructor hidden (but see `sorry`)
data Proof p = QED deriving (Functor, Applicative, Monad)

-- Attaching predicates to values
newtype a ::: p = SuchThat Defn

because :: a -> Proof p -> (a ::: p)
x `because` p = coerce x

-- Logical constants
data TRUE
data FALSE
data p && q
data p || q
data p --> q
data Not p

-- Local inference rules (implementations all
-- ignore parameters and return `QED`)
and_intro   :: p -> q -> Proof (p && q)
or_elimL    :: (p || q) -> Proof p
impl_intro  :: (p -> Proof q) -> Proof (p --> q)
contradicts :: p -> Not p -> Proof FALSE
absurd      :: FALSE -> Proof p
  -- ... and many more

-- Proof combinators (specialized from Control.Monad)
-- Carefuly fixity definitions allow easy composition
-- of linear proofs.
(|$) :: p -> (p -> Proof q) -> Proof q 
(|.) :: (p -> Proof q)
     -> (q -> Proof r) -> (p -> Proof r)
(|/) :: ((p -> Proof q) -> Proof r)
     -> (p -> Proof q) -> Proof r
(|\) :: (p -> Proof q)
     -> ((p -> Proof q) -> Proof r) -> Proof r

-- Exported function that allowis library authors to
-- sassert arbitrary axioms about their API.
sorry :: Proof p
sorry = QED
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{pred.hs}
  \caption{Basic constants and functions for building up the ``proofs''
    in ``ghosts of departed proofs''. 
    \label{predicate-logic}}
\end{figure}

\begin{figure*}[t!]
  \hspace{-2.5in}
  \scalebox{0.6}{{
  \begin{minipage}{\textwidth}
    \begin{prooftree}
      \AxiomC{}
      \RightLabel{\scriptsize (p)}\UnaryInfC{$\texttt{IsNil}(x) \wedge |x| = 0$}
      \UnaryInfC{$|x| = 0$}
      \UnaryInfC{$0 = |x|$}
      \AxiomC{}
      \RightLabel{\scriptsize (eq)}\UnaryInfC{$|x| = 1 + n$}
      \BinaryInfC{$0 = 1 + n$}
      \AxiomC{}
      \UnaryInfC{$\forall m \in \mathbb{N}.~ \neg (0 = 1 + m)$}
      \UnaryInfC{$\neg (0 = 1 + n)$}
      \BinaryInfC{$\bot$}
      \UnaryInfC{$\texttt{IsCons}(x)$}
      \RightLabel{\scriptsize (p)}\UnaryInfC{$\texttt{IsNil}(x) \wedge |x| = 0 \longrightarrow \texttt{IsCons}(x)$}
      \AxiomC{}
      \RightLabel{\scriptsize (q)}\UnaryInfC{$\texttt{IsCons}(x) \wedge |x| = 1 + |\texttt{Tail}(x)|$}
      \UnaryInfC{$\texttt{IsCons}(x)$}
      \RightLabel{\scriptsize (q)}\UnaryInfC{$\texttt{IsCons}(x) \wedge |x| = 1 + |\texttt{Tail}(x)| \longrightarrow \texttt{IsCons}(x)$}
      \AxiomC{}
      \UnaryInfC{$\forall \ell \in \texttt{List}_T.~ (\texttt{IsNil}(\ell) \wedge |\ell| = 0) \vee (\texttt{IsCons}(\ell) \wedge |\ell| = 1 + |\texttt{Tail}(\ell)|)$}
      \UnaryInfC{$(\texttt{IsNil}(x) \wedge |x| = 0) \vee (\texttt{IsCons}(x) \wedge |x| = 1 + |\texttt{Tail}(x)|)$}
      \TrinaryInfC{$\texttt{IsCons}(x)$}
      \RightLabel{\scriptsize (eq)}\UnaryInfC{$|x| = 1 + n \longrightarrow \texttt{IsCons(x)}$}
    \end{prooftree}
  \end{minipage}
    }
  }
  \caption{A proof that lists with nonzero length satisfy the \texttt{IsCons} predicate,
      in natural deduction style. Compare with the same proof using the \texttt{Proof} monad
      in listing ****; the steps after the \texttt{(|/)} operator correspond to the leftmost
      deductions in this proof tree. Note a slight difference: the listing proves
      $|x| = 1 + n \vdash \texttt{IsCons}(x)$, while the derivation in this figure
      proves $\vdash |x| = 1 + n \longrightarrow \texttt{IsCons}(x)$.}
\end{figure*}

\begin{filecontents*}{nzlic.hs}
nonzero_length_implies_cons
  :: (Length xs == Succ n) -> Proof (IsCons xs)

nonzero_length_implies_cons eq =
  do  toSpec length
   |$ or_elimR and_elimL
   |/ and_elimR
   |. symmetric
   |. transitive' eq
   |. (contradicts' $$ zero_not_succ)
   |. absurd
\end{filecontents*}

\inputminted{haskell}{nzlic.hs}

\subsection{Adding proof tactics}

For simple properties, the task of writing a proof is not too difficult. But for
more sophisticated properties, the deployment of \emph{proof tactics} becomes
critical. A proof tactic is a search strategy for proofs, usually targeted at
proving one particular class of theorems. For example, the \texttt{Coq} tactic
\texttt{omega} is useful for proving theorems about arithmetic, while
\texttt{simpl} is useful for simplifying a complex goal.

Tactics are often designed with a specific domain in mind; to be most useful,
theory creators (and library authors) should be able to create their own tactics
when needed. For example, a library dealing extensively with fixed-width numeric types
may benefit from specific proof tactics based around the theory of bitvectors.

One approach to providing custom tactics is to leverage \texttt{GHC}'s support for
\emph{type-checker plugins}. These plugins hook into \texttt{GHC}'s $\textsf{OutsideIn}(X)$
inference algorithm, extending the algorithm's capabilities.

As a proof-of-concept, we developed a simple typechecker plugin that implements
proof by analytic tableaux \cite{smullyan1995first}. This tactic can verify the satisfiability of any
valid formula of propositional logic; the na\"ive implementation takes about
60 lines of Haskell, plus 150 lines of glue code to mediate between
the tableaux solver and $\textsf{OutsideIn}(X)$.

To trigger the custom tactic, we introduce an empty injective type family---hidden
from the user---and a single exported function \texttt{tableaux}.
\begin{minted}{haskell}
type family ProofByTableaux p = p' | p' -> p

tableaux :: ProofByTableaux p
tableaux = error "proof by analytic tableaux."
\end{minted}
Morally, we want to think of \texttt{ProofByTableaux p} as an alias for \texttt{p}.
By introducing it as an empty type family, we will give our type checker plugin a
********

The type-checker plugin will get a chance to intervene whenever \texttt{GHC}
attempts to solve a type equality constraint of the form \texttt{ProofByTableaux p $\sim$ p'}.
When such a constraint is met, the plugin performs the following tasks:
\begin{enumerate}
\item Convert the \emph{type} \texttt{p'} to a \emph{formula} $\Phi$ in propositional
  logic, introducing free variables for any subterms that are not built from the
  propositional logic type constructors \texttt{And}, \texttt{Or}, and so on.
\item Invoke the solver for analytic tableaux, attempting to find an assignment of truth
  values to variables such that $\neg \Phi$ is true.
\item If the solver finds such a truth assignment, report an error: the user asked us
  to apply the tactic to prove $\Phi$, but the variable assignment demonstrated that
  $\Phi$ is actually \emph{false}.
\item If the solver finds that no such truth assignment exists, then we have proven that
  $\Phi$ is valid. Tell \texttt{GHC} to discharge the constraint \texttt{ProofByTableaux p $\sim$ p'}, replacing it with \texttt{p $\sim$ p'}.
\end{enumerate}
For the user, the effect appears to be that \texttt{tableaux} can act as a value of
type \texttt{Proof p} whenever \texttt{p} is a valid formula in propositional logic.

\begin{filecontents*}{tableaux.hs}
proof1, proof2 :: Proof (p #$\wedge$# q --> p #$\vee$# q)
  
proof1 =  and_elimL
       |. or_introL
       |\ impl_intro

proof2 = tableaux
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{tableaux.hs}
  \caption{Custom proof tactics can be implemented as type-checker plugins and
    included with libraries. Here, a proof requirement is discharged in two ways:
    once by hand, and then again by appeal to the \texttt{tableaux} plugin.
    \label{tableaux-example}}
\end{figure}

\subsection{Refinement types via phantom predicates}

(cite the \texttt{refined} library here) \cite{refined}


\section{Warm-up: Faux dependent types}

\begin{filecontents*}{ex1.hs}  
{-# LANGUAGE RankNTypes #-}
module Sized (Size, the, sZipWith, sizing, align) where

newtype List n a = List a

the :: List n a -> a
the (List x) = x

sZipWith ::
  (a -> b -> c) -> List n a -> List n b -> List n c
sZipWith f xs ys = List (zipWith f (the xs) (the ys))
  
sizing :: [a] -> (forall n. List n a -> t) -> t
sizing xs k = k (List xs)

align :: List n a -> [b] -> Maybe (List n b)
align xs ys = if length (the xs) == length ys
              then Just (List ys)
              else Nothing
\end{filecontents*}

\begin{filecontents*}{ex2.hs}
import Sized

dot :: List n Double -> List n Double -> Double
dot xs ys = sum (the $ sZipWith (*) xs ys)

dot :: ([Double] ~~ xs)
    -> ([Double] ~~ ys ::: Length xs == Length ys)
    -> Double
dot xs ys = sum (the $ sZipWith (*) xs ys)
    
main :: IO ()
main = do
  xs <- readLn
  ys <- readLn
  sizing xs $ \xs' -> do
    case align xs' ys of
      Nothing  -> putStrLn "Size mismatch!"
      Just ys' -> print (dot xs' ys')
\end{filecontents*}

\begin{figure}
    \inputminted{haskell}{ex1.hs}
  \caption{A small module defining a type for lists with a known length. \label{sizing}}
\end{figure}
\begin{figure}
    \inputminted{haskell}{ex2.hs}
    \caption{A user-defined dot product function that can only be used on same-sized lists,
      and a usage example. The types of \texttt{xs'} and \texttt{ys'} ensure that they have
      the same length, when both variables are in scope.\label{dot-product}}
\end{figure}

Despite  appearences, the phantom type parameter $n$ does not really represent the vector's length
{\em per se}. Instead, we propose to think of the phantom type \texttt{p} in \texttt{List p a} as a predicate,
and values of type \texttt{List n a} as ``lists of type \texttt{[a]}, equipped with a proof
that they satisfy \texttt{p}''. Critically, this proof has no run-time impact: it is trapped in the
phantom type parameter, and carried by a \texttt{newtype} wrapper that vanishes in
the final compilation.


As we attach increasingly sophisticated information into the phantom types, it becomes useful to
have a uniform method for \emph{forgetting} all of the ornamentation, revealing the normal
value underneath.

In \Cref{dot-product}, note that the error handler for mismatched list sizes happens
\emph{outside} of the dot product function. At the point where the \texttt{dot} function
is actually invoked, the programmer has already provided evidence that the operation is
safe, in the form of the phantom type parameters on \texttt{xs'} and \texttt{ys'}. This pattern
fits well with a common approach to \emph{using} libraries: first, the user does input scrubbing
and validation, and only then are the validated inputs are sent on to the main computations. 
During validation, the user is proving that the library's prerequisites have been met. The
main computations can then be written **** 

\subsection{A feint towards dependent types}
So far, the \texttt{Sized} API allows us to assert that two lists are known to have the same
length, and to introduce an existentially-quantified (but unknown) size for a ``regular'' list.
As the authors of the \texttt{Sized} module, we can introduce more interesting variations as
well. For example, \Cref{size-evidence} shows one way to add types that represent a length of
zero, or an even length, or the sum of two other lenghts. ***

\begin{filecontents*}{more_lists.hs}
data Zero    -- no constructors, ghosts need no body
data NonZero
data x + y

nil :: ([a] ~~ Nil)
nil = defn []

gdpHead :: ([a] ~~ xs ::: IsCons xs) -> (a ~~ Head xs)
gdpHead xs = defn (head (the xs)) -- safe!

gdpRev :: ([a] ~~ xs) -> ([a] ~~ Reverse xs)
gdpRev = defn . reverse . coerce

revCons :: ([a] ~~ xs ::: IsCons xs)
        -> ([a] ~~ Reverse xs ::: IsCons (Reverse xs))
revCons = defn . reverse . coerce
        
gdpLen :: ([a] ~~ xs) -> (Integer ~~ Length xs)
gdpLen xs = defn (length (the xs))

classify :: ([a] ~~ xs) -> Either (Proof (IsCons xs)) (Proof (IsNil xs))
classify xs = case xs of
  []    -> Left sorry
  (_:_) -> Right sorry
  
(+++) :: ([a] ~~ xs)
      -> ([a] ~~ ys)
      -> ([a] ~~ xs +++ ys)
xs +++ ys = defn (the xs ++ the ys)

-- Lemmas
length_spec :: Proof ( (IsNil xs && Length xs == Zero)
  || (IsCons xs && Length xs == Succ (Length (Tail xs))) )

length_sum :: Proof (Length (xs +++ ys) == Length xs + Length ys)
length_Sum = sorry

rev_length :: Proof (Length (Reverse xs) == Length xs)
rev_length = sorry

zero_neutral :: Proof (n == Zero + n)
zero_neutral = sorry

add_commutes :: Proof (n + m == m + n)
add_commutes = sorry
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{more_lists.hs}
  \caption{Extending the \texttt{Sized} module with additional size predicates,
    functions, and proof combinators for manipulating size evidence.\label{size-evidence}}
  \end{figure}

Except in the simplest of cases, it is important that the library author equip the
user with a selection of \emph{proof combinators} for changing the shape of the
property captured by the phantom type parameter. In \Cref{size-evidence}, \texttt{zero\_neutral}
and \texttt{add\_commutes} are proof combinators that allow the user to convert
evidence from one form to another.

\begin{minted}[frame=none]{haskell}
head' :: ([a] ~~ xs ::: ) -> a
head' xs = gdpHead ((nil +++ xs) ...ok)
  where ok =  zero_neutral
           |. symmetric

norm2 :: [Double] -> Double
norm2 xs = name xs $ \xs' -> dot xs' ((xs' +++ nil) ...ok)
  where ok = zero_neutral . add_commutes
\end{minted}

*** It is not so much effort to introduce a \texttt{NonEmpty} type for lists, but it
would be intractable to add \texttt{EvenSizedList}, \texttt{InfiniteList},
\texttt{EmptyList}, and so on. Each library function would need a variant
for each new type, or else a carefully curated collection of typeclasses would
be required to provide \emph{ad hoc} overloading. By contrast, the GDP approach
allows the same library functions on a wide variety of *******.

%% \subsection{Infinite lists}

%% \begin{minted}{haskell}
%% data Infinite

%% omega :: List Infinite Int
%% omega = List [0..]

%% pop :: List Infinite a -> (a, List Infinite a)
%% pop (List xs) = (head xs, List (tail xs))
%% \end{minted}

   
\subsection{Limitations}
As alluded to in the section heading, the GDP approach gives us
types that have \emph{some} of the features of dependent types, but
they are strictly less powerful. For example, we cannot
write a dependently-typed \texttt{replicate} function of the form
\texttt{replicate : (n : $\mathbb{N}$) -> A -> Vec A n}

\subsection{The \texttt{The} typeclass}
\begin{filecontents*}{theTC.hs}
class The d a | d -> a where
    the :: d -> a
    default the :: Coercible d a => d -> a
    the = coerce
    
instance The (List n a) a
\end{filecontents*}

\begin{figure}
  \inputminted{haskell}{theTC.hs}
  \caption{The \texttt{The} typeclass, for dropping ghosts
    from a type. The default instance should always be used,
    so new instances can be created with an empty
    \texttt{instance} declaration.}
\end{figure}

\bibliographystyle{abbrvnat}

\bibliography{gdp.bib}
%% \begin{thebibliography}{10}
%%   \softraggedright
  
%% \bibitem[Berkeley (1734) Berkeley]{analyst}
%%   G. Berkeley. \newblock The Analyst, a Discourse ADdressed to an Infidel Mathematician.
%%   \newblock 1734.
%% \end{thebibliography}


\end{document}
